-- Databricks notebook source
-- MAGIC %md # Introduction to Surrogate Key Generation for Lake House
-- MAGIC 
-- MAGIC 1. History of Surrogate Keys
-- MAGIC 2. Why is it hard
-- MAGIC 3. Requirements for a good strategy
-- MAGIC   * Embarisingly parallel
-- MAGIC   * SQL based
-- MAGIC   * No Collisions
-- MAGIC 
-- MAGIC 4. Strategies
-- MAGIC   * monotonically_increasing_id
-- MAGIC   * row_number() Rank OVER
-- MAGIC   * ZipWithIndex()
-- MAGIC   * ZipWithUniqueIndex()
-- MAGIC   * Row Hash with hash()
-- MAGIC   * Row Hash with md5()
-- MAGIC 
-- MAGIC   
-- MAGIC 5. Validation
-- MAGIC   * Uniqueness / Collisions
-- MAGIC   * Distributability (Performance)
-- MAGIC   * Distribution (Skewness)
-- MAGIC 
-- MAGIC 5. Summary
-- MAGIC * Performance chart

-- COMMAND ----------

-- MAGIC %md ## History of Surrogate Keys
-- MAGIC "A surrogate key (or synthetic key, entity identifier, system-generated key, database sequence number, factless key, technical key, or arbitrary unique identifier[citation needed]) in a database is a unique identifier for either an entity in the modeled world or an object in the database. The surrogate key is not derived from application data, unlike a natural (or business) key which is derived from application data.[1]"
-- MAGIC https://en.wikipedia.org/wiki/Surrogate_key
-- MAGIC 
-- MAGIC [Ralph Kimbal](https://www.kimballgroup.com/1998/05/surrogate-keys/):
-- MAGIC "Actually, a surrogate key in a data warehouse is more than just a substitute for a natural key. In a data warehouse, a surrogate key is a necessary generalization of the natural production key and is one of the basic elements of data warehouse design. Let’s be very clear: Every join between dimension tables and fact tables in a data warehouse environment should be based on surrogate keys, not natural keys. It is up to the data extract logic to systematically look up and replace every incoming natural key with a data warehouse surrogate key each time either a dimension record or a fact record is brought into the data warehouse environment."
-- MAGIC 
-- MAGIC "A surrogate key is frequently a sequential number (e.g. a Sybase or SQL Server "identity column", a PostgreSQL or Informix serial, an Oracle or SQL Server SEQUENCE or a column defined with AUTO_INCREMENT in MySQL). Some databases provide UUID/GUID as a possible data type for surrogate keys (e.g. PostgreSQL UUID or SQL Server UNIQUEIDENTIFIER)."
-- MAGIC 
-- MAGIC Approaches to generating surrogates include:
-- MAGIC 
-- MAGIC * Universally Unique Identifiers (UUIDs)
-- MAGIC * Globally Unique Identifiers (GUIDs)
-- MAGIC * Object Identifiers (OIDs)
-- MAGIC * Sybase or SQL Server identity column IDENTITY OR IDENTITY(n,n)
-- MAGIC * Oracle SEQUENCE, or GENERATED AS IDENTITY (starting from version 12.1)[3]
-- MAGIC * SQL Server SEQUENCE (starting from SQL Server 2012)[4]
-- MAGIC * PostgreSQL or IBM Informix serial
-- MAGIC * MySQL AUTO_INCREMENT
-- MAGIC * SQLite AUTOINCREMENT
-- MAGIC * AutoNumber data type in Microsoft Access
-- MAGIC * AS IDENTITY GENERATED BY DEFAULT in IBM DB2
-- MAGIC * Identity column (implemented in DDL) in Teradata
-- MAGIC * [SURROGATE_KEY user-defined function (UDF) in Hive HDP 3.5](https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/using-hiveql/content/hive_surrogate_keys.html)
-- MAGIC * [ZipWithUniqueId() using RDDs in Spark 1.6](https://spark.apache.org/docs/1.6.1/api/python/pyspark.html#pyspark.RDD.zipWithUniqueId)
-- MAGIC * [ZipWithIndex() using RDDs in Spark 1.6](https://spark.apache.org/docs/1.6.1/api/python/pyspark.html#pyspark.RDD.zipWithUniqueId)

-- COMMAND ----------

-- MAGIC %md ## Why are Surrogate Keys Hard?
-- MAGIC 
-- MAGIC We're going back to the past...and getting a tad geeky (apologies in advance...not really).
-- MAGIC 
-- MAGIC ### SKs are hard even for single node (traditional) data warehousing 
-- MAGIC 
-- MAGIC In databases, using increment/identity/GUID/UUIDs (*SK*) allows you to generate a unique surrogate key for that database.  If you sharded the databases (i.e. multiple databases with the same schema but would process a different shard of data), the *SK* would be unique for only that database.  This could be mitigated by adding application code (or joins) that concatenated/joined the shard database identifier with the *SK* but this would have an undesirable performance impact.  
-- MAGIC 
-- MAGIC Even if this approach worked, what if the same *identifier* showed up in two different shards; this would result in different SKs for the same *identifier* which would be undesireable business outcome. For example:
-- MAGIC 
-- MAGIC 
-- MAGIC * Initially, Samantha Carter's is based on Earth (Tau'ri) and part SG-1 team. 
-- MAGIC > <img src="https://ladygeekgirl.files.wordpress.com/2014/10/sam-carter.jpg" width="200"/>
-- MAGIC 
-- MAGIC * After 15 years successfully saving Milky Way galaxy, she is now leading the Atlantis team in the Pegasus galaxy.
-- MAGIC > <img src="https://static.tvmaze.com/uploads/images/medium_portrait/1/3068.jpg" width="200"/>
-- MAGIC 
-- MAGIC * In this example, the *identifier* is the PK and Name (technically just the name) and we're updating Carter's demographic information.
-- MAGIC 
-- MAGIC | Shard | SK | PK | Name | Team | Location | Point of Origin |
-- MAGIC | ----- | -- | -- | ---- | ---- | -------- | --------------- |
-- MAGIC | 0 | 100 | 1 | Samantha Carter | SG-1 | Tau'ri | <img src="https://vignette.wikia.nocookie.net/stargate/images/7/72/0Ega.svg/revision/latest/scale-to-width-down/185?cb=20110924121417" width=48/> |
-- MAGIC | 0 | 102 | 1 | Samantha Carter | Atlantis | Subido | <img src="https://vignette.wikia.nocookie.net/stargate/images/c/c6/A04.svg/revision/latest?cb=20070310020549" width=48/> |
-- MAGIC | 3 | 1182 | 1 | Samantha Carter | Atlantis | Subido | <img src="https://vignette.wikia.nocookie.net/stargate/images/c/c6/A04.svg/revision/latest?cb=20070310020549" width=48/> |
-- MAGIC 
-- MAGIC * But how about if our sharded databases received Carter's updates at the same time and it was not distributed properly (either due to design, scale, multi-verse, etc.)?  This would result in logically two different surrogate keys for Carter.   
-- MAGIC 
-- MAGIC * As noted, this is undesirable as you would want to properly associate the same SK to the same *identifier* - i.e. to Carter.  And while this may seem obvious to avoid now, this was a problem with traditional databases.
-- MAGIC 
-- MAGIC 
-- MAGIC ### Perhaps let's centralize it all?
-- MAGIC One short-lived approach was to have a central server or service where the only job was to provide unique SKs for all values. The problem with this approaches included:
-- MAGIC * As the database transactions increased, the higher the load to generate the SKs.
-- MAGIC * Even if it could handle the SK *creation*, it became resource intensive for the SK *lookup*. 
-- MAGIC * It ultimately required the a resource intensive OLTP database to continually generate or lookup values based on a provided *identifier* saturating the server resources or network in between.
-- MAGIC 
-- MAGIC 
-- MAGIC ### Let's Hash our way of this
-- MAGIC 
-- MAGIC A potential solution for this would be to create a hash of the *identifier* (often multiple columns).  The key advantage of this approach is that provided the same hash (and keys) are used, any sharded database (or distributed system for that matter) could get the exact same hash value if they were using the same identifier.  Thus, if you had two shards, they would run the same *hash* function and get something like the value below.
-- MAGIC 
-- MAGIC \\[hash([A, B]) = hash([1, \`Samantha\: Carter\`]) = BC304 \\]
-- MAGIC 
-- MAGIC The potential problem with this approach though is **hash collisions** where the two different inputs results in the same output hash value, e.g.
-- MAGIC 
-- MAGIC 
-- MAGIC \\[hash([A, B]) = hash([1, \`Samantha\: Carter\`, \`SG-1\`]) = X303 \\]
-- MAGIC \\[hash([A, B]) = hash([1, \`Daniel\: Jackson\`, \`SG-1\`]) = X303 \\]
-- MAGIC 
-- MAGIC This issue is also known as the "birthday problem" where two different UserIDs having the same Hash_of_the_UserID.   For example, if you use a 32-bit hash (the equivalent of converting your RDBMS HashBytes hash value to integer) with 100,000 users, we can use the Taylor series to approximate the chance (p) of a collision:
-- MAGIC  
-- MAGIC \\[ p = 1 - e^(\frac{-1 x (10^5)^2}{2 x 2^{32}}) = 68.7871\% \\]
-- MAGIC 
-- MAGIC If you use a 64-bit hash (the equivalent of converting your RDBMS HashBytes hash value to big integer), the chance of collision essentially becomes zero for 100,000 users. It isn’t until you reach 100 million users that the chance of collision climbs to 0.027%. 
-- MAGIC 
-- MAGIC Note, The Taylor series approximates the chance of only one collision. To determine the chances of more than one collision, a general approximation is to use the binomial distribution based on the presumption that a good hash function imitates a uniform random spread over the hash space (that is, every selection of a hash value has a fixed chance of hitting something already in the set). Therefore, when using a 32-bit hash with 100,000 customers, the success probability of 2 hash collisions is 26.42% and the success probability of >= 2 hash collisions is 67.56%!
-- MAGIC 
-- MAGIC In the past, we would solve the problem by using 64-bit hashes to minimize the likelihood of collisions.  When working with hundreds of millions of hashes, admittedly the likelihood of collision is pretty low (0.027%).  But how if you're working with billions or trillions of hashes?  Distribution to the rescue...not really!

-- COMMAND ----------

-- MAGIC %md
-- MAGIC ### SKs can be even harder for distributed systems
-- MAGIC When working with even larger datasets, it can be even more difficult to ensure uniqueness for your surrogate keys while at the same time ensuring that any node that sees the same identifier generates the same hash value.  The **requirements for a good surrogate key** are
-- MAGIC 
-- MAGIC * More unique identifiers resulting in higher probability of hash collions
-- MAGIC * To minimize this, increase the size of the hash and/or use different algorithms, e.g. in Apache Spark 
-- MAGIC  * MD5: 128-bit
-- MAGIC  * SHA2: SHA-224, SHA-224, SHA-256, SHA-384, and SHA-512 are supported
-- MAGIC * The larger or more complicated the hash, the larger the potential performance impact.  Note, this is not as much of an issue as in the past but you still do want to be on the look out for this. 
-- MAGIC 
-- MAGIC 
-- MAGIC #### Don't forget about partitioning!!
-- MAGIC With all the focus on generating hashes for uniqueness and speed, often forgotten is how you intend to partition this data?  
-- MAGIC * If you're not partitioning by the SK but by something else (e.g userID), you *should* be okay provided there is a 1:few relationship between otherwise you may end up with a massive skew (e.g. NULL value overload)
-- MAGIC * If you are partitioning by the SK, are the values actually random (e.g. GUID is unique but not necessarily random)
-- MAGIC * You could use range partitioning if you can generate and stay within a range of values

-- COMMAND ----------

-- MAGIC %md # Demo Setup

-- COMMAND ----------

-- DBTITLE 1,Set test parameters
SET test.nrows = 1000000;
SET test.npartitions = 512;

-- COMMAND ----------

-- MAGIC %md # Test monotonically_increasing_id()

-- COMMAND ----------

-- MAGIC %md ### Failure scenario
-- MAGIC * Insert
-- MAGIC * Insert

-- COMMAND ----------

CREATE OR REPLACE TABLE TEST_DIM (
 SK BIGINT COMMENT 'SURROGATE KEY',
 ID BIGINT COMMENT 'Identifier',
 random_number DOUBLE COMMENT "Random number",
 str1 STRING COMMENT 'STRING VAL 1',
 str2 STRING COMMENT 'STRING VAL 2',
 str3 STRING COMMENT 'STRING VAL 3'
) USING DELTA
LOCATION 'dbfs:/tmp/test_dim_remove_me'

-- COMMAND ----------

-- DBTITLE 1,Insert Data
WITH sample_data (
  SELECT
    id,
    rand() random_number,
    'one one one' str1,
    'two one one' str2,
    'three one one' str3
  FROM
    RANGE(0, ${test.nrows}, 1, ${test.npartitions})  -- start, end, step, numPartitions [Set to a multiple of the cores]
)
INSERT INTO TEST_DIM
SELECT monotonically_increasing_id() as sk, * FROM sample_data

-- COMMAND ----------

SELECT * FROM TEST_DIM

-- COMMAND ----------

WITH sample_data (
  SELECT
    id,
    rand() random_number,
    'one one one' str1,
    'two one one' str2,
    'three one one' str3
  FROM
    RANGE(0, ${test.nrows}, 1, ${test.npartitions})  -- start, end, step, numPartitions 
)
INSERT INTO TEST_DIM
SELECT monotonically_increasing_id() as sk, * FROM sample_data

-- COMMAND ----------

-- DBTITLE 1,Should be equal if successful, this one is broken
SELECT count(*), count(DISTINCT sk) FROM TEST_DIM

-- COMMAND ----------

-- MAGIC %md ### Success scenario
-- MAGIC * Insert
-- MAGIC * Get max SK value
-- MAGIC * Insert w/ offset applied to SK

-- COMMAND ----------

-- DBTITLE 1,Reset
CREATE OR REPLACE TABLE TEST_DIM (
 SK BIGINT COMMENT 'SURROGATE KEY',
 ID BIGINT COMMENT 'Identifier',
 random_number DOUBLE COMMENT "Random number",
 str1 STRING COMMENT 'STRING VAL 1',
 str2 STRING COMMENT 'STRING VAL 2',
 str3 STRING COMMENT 'STRING VAL 3'
) USING DELTA
LOCATION 'dbfs:/tmp/test_dim_remove_me'

-- COMMAND ----------

WITH sample_data (
  SELECT
    id,
    rand() random_number,
    'one one one' str1,
    'two one one' str2,
    'three one one' str3
  FROM
    RANGE(0, ${test.nrows}, 1, ${test.npartitions})  -- start, end, step, numPartitions 
)
INSERT INTO TEST_DIM
SELECT monotonically_increasing_id() as sk, * FROM sample_data

-- COMMAND ----------

-- MAGIC %md #### Get Max SK value
-- MAGIC * Get the max SK value of all previous inserts
-- MAGIC * This is a relatively fast metadata operation on Delta Lake tables (min/max is stored in the delta log)
-- MAGIC * This is not atomic

-- COMMAND ----------

WITH sample_data (
  SELECT
    id,
    rand() random_number,
    'one one one' str1,
    'two one one' str2,
    'three one one' str3
  FROM
    RANGE(0, ${test.nrows}, 1, ${test.npartitions})  -- start, end, step, numPartitions 
)
INSERT INTO TEST_DIM
SELECT monotonically_increasing_id() + 1 + (select max(sk) from TEST_DIM) as sk, * FROM sample_data

-- COMMAND ----------

-- DBTITLE 1,Succeeds when count() and count(distinct()) equal
SELECT count(*), count(DISTINCT sk)FROM TEST_DIM

-- COMMAND ----------

-- DBTITLE 1,However, look at max(sk)
SELECT min(sk), max(sk) FROM TEST_DIM

-- COMMAND ----------

select sk from TEST_DIM

-- COMMAND ----------

-- DBTITLE 1,Succeeds when there are no duplicate SK values
select count(*), sk from TEST_DIM group by sk having count(*) > 1

-- COMMAND ----------

-- MAGIC %md ## Evaluation of monotonically_increasing_id()
-- MAGIC * Uniqueness
-- MAGIC * Evenly Distributed
-- MAGIC * DBA Perspective
-- MAGIC * Fact & Dimension table partitioning SK (spark performance)

-- COMMAND ----------

-- MAGIC %md # Test zipWithIndex()
-- MAGIC [Spark 1.6 Docs](https://spark.apache.org/docs/1.6.1/api/python/pyspark.html#pyspark.RDD.zipWithIndex)
-- MAGIC 
-- MAGIC Zips this RDD with its element indices.
-- MAGIC 
-- MAGIC The ordering is first based on the partition index and then the ordering of items within each partition. So the first item in the first partition gets index 0, and the last item in the last partition receives the largest index.
-- MAGIC 
-- MAGIC This method needs to trigger a spark job when this RDD contains more than one partitions.
-- MAGIC 
-- MAGIC This example will use zipWithIndex() to generate IDs over a Dataframe

-- COMMAND ----------

-- DBTITLE 1,dfZipWithIndex helper function
-- MAGIC %python
-- MAGIC from pyspark.sql.types import LongType, StructField, StructType
-- MAGIC 
-- MAGIC def dfZipWithIndex (df, offset=1, colName="rowId"):
-- MAGIC     '''
-- MAGIC         Ref: https://stackoverflow.com/questions/30304810/dataframe-ified-zipwithindex
-- MAGIC         Enumerates dataframe rows is native order, like rdd.ZipWithIndex(), but on a dataframe 
-- MAGIC         and preserves a schema
-- MAGIC 
-- MAGIC         :param df: source dataframe
-- MAGIC         :param offset: adjustment to zipWithIndex()'s index
-- MAGIC         :param colName: name of the index column
-- MAGIC     '''
-- MAGIC 
-- MAGIC     new_schema = StructType(
-- MAGIC                     [StructField(colName,LongType(),True)]        # new added field in front
-- MAGIC                     + df.schema.fields                            # previous schema
-- MAGIC                 )
-- MAGIC 
-- MAGIC     zipped_rdd = df.rdd.zipWithIndex()
-- MAGIC 
-- MAGIC     new_rdd = zipped_rdd.map(lambda row: ([row[1] +offset] + list(row[0])))
-- MAGIC 
-- MAGIC     return spark.createDataFrame(new_rdd, new_schema)

-- COMMAND ----------

CREATE OR REPLACE TABLE TEST_DIM (
 SK BIGINT COMMENT 'SURROGATE KEY',
 ID BIGINT COMMENT 'Identifier',
 random_number DOUBLE COMMENT "Random number",
 str1 STRING COMMENT 'STRING VAL 1',
 str2 STRING COMMENT 'STRING VAL 2',
 str3 STRING COMMENT 'STRING VAL 3'
) USING DELTA
LOCATION 'dbfs:/tmp/test_dim_remove_me'

-- COMMAND ----------

-- MAGIC %python
-- MAGIC 
-- MAGIC df = spark.sql("""
-- MAGIC   SELECT
-- MAGIC     id,
-- MAGIC     rand() random_number,
-- MAGIC     'one one one' str1,
-- MAGIC     'two one one' str2,
-- MAGIC     'three one one' str3
-- MAGIC   FROM
-- MAGIC     RANGE(0, ${test.nrows}, 1, ${test.npartitions})
-- MAGIC """)
-- MAGIC 
-- MAGIC df_indexed = dfZipWithIndex(df, offset=1, colName="sk")
-- MAGIC df_indexed.createOrReplaceTempView("CHANGE_SET1")
-- MAGIC spark.sql("""INSERT INTO TEST_DIM SELECT * FROM CHANGE_SET1""")

-- COMMAND ----------

-- MAGIC %python
-- MAGIC N = spark.sql("select max(sk) from TEST_DIM").collect()[0][0]
-- MAGIC spark.conf.set("test.max_sk", N)  # set context variable for use in next insert batch
-- MAGIC print(spark.conf.get("test.max_sk"))

-- COMMAND ----------

-- MAGIC %python
-- MAGIC 
-- MAGIC df = spark.sql("""
-- MAGIC   SELECT
-- MAGIC     id,
-- MAGIC     rand() random_number,
-- MAGIC     'one one one' str1,
-- MAGIC     'two one one' str2,
-- MAGIC     'three one one' str3
-- MAGIC   FROM
-- MAGIC     RANGE(0, ${test.nrows}, 1, ${test.npartitions})
-- MAGIC """)
-- MAGIC 
-- MAGIC df_indexed = dfZipWithIndex(df, offset=N+1, colName="sk")
-- MAGIC df_indexed.createOrReplaceTempView("CHANGE_SET2")
-- MAGIC spark.sql("""INSERT INTO TEST_DIM SELECT * FROM CHANGE_SET2""")

-- COMMAND ----------

SELECT format_number(count(*),0), format_number(count(DISTINCT sk),0) FROM TEST_DIM

-- COMMAND ----------

SELECT min(sk), format_number(max(sk),0) FROM TEST_DIM

-- COMMAND ----------

-- MAGIC %md # Test ZipWithUniqueIndex
-- MAGIC [Spark 1.6 Docs](https://spark.apache.org/docs/1.6.1/api/python/pyspark.html#pyspark.RDD.zipWithUniqueId)
-- MAGIC 
-- MAGIC Zips this RDD with generated unique Long ids.
-- MAGIC * Items in the kth partition will get ids k, n+k, 2*n+k, ..., 
-- MAGIC   * where n is the number of partitions. So there may exist gaps, but this method won’t trigger a spark job, which is different from zipWithIndex
-- MAGIC 
-- MAGIC * Classic advice for Spark 1.x / RDDs for generating unique IDs
-- MAGIC 
-- MAGIC This example will use zipWithUniqueId() to generate IDs over a Dataframe

-- COMMAND ----------

-- DBTITLE 1,dfZipWithUniqueIndex helper function
-- MAGIC %python
-- MAGIC from pyspark.sql.types import LongType, StructField, StructType
-- MAGIC 
-- MAGIC def dfZipWithUniqueIndex (df, offset=1, colName="rowId"):
-- MAGIC     '''
-- MAGIC         Ref: https://stackoverflow.com/questions/30304810/dataframe-ified-zipwithindex
-- MAGIC         Enumerates dataframe rows is native order, like rdd.ZipWithUniqueIndex(), but on a dataframe 
-- MAGIC         and preserves a schema
-- MAGIC 
-- MAGIC         :param df: source dataframe
-- MAGIC         :param offset: adjustment to zipWithIndex()'s index
-- MAGIC         :param colName: name of the index column
-- MAGIC     '''
-- MAGIC 
-- MAGIC     new_schema = StructType(
-- MAGIC                     [StructField(colName,LongType(),True)]        # new added field in front
-- MAGIC                     + df.schema.fields                            # previous schema
-- MAGIC                 )
-- MAGIC 
-- MAGIC     zipped_rdd = df.rdd.zipWithUniqueId()
-- MAGIC 
-- MAGIC     new_rdd = zipped_rdd.map(lambda row: ([row[1] +offset] + list(row[0])))
-- MAGIC 
-- MAGIC     return spark.createDataFrame(new_rdd, new_schema)

-- COMMAND ----------

CREATE OR REPLACE TABLE TEST_DIM (
 SK BIGINT COMMENT 'SURROGATE KEY',
 ID BIGINT COMMENT 'Identifier',
 random_number DOUBLE COMMENT "Random number",
 str1 STRING COMMENT 'STRING VAL 1',
 str2 STRING COMMENT 'STRING VAL 2',
 str3 STRING COMMENT 'STRING VAL 3'
) USING DELTA
LOCATION 'dbfs:/tmp/test_dim_remove_me'

-- COMMAND ----------

-- MAGIC %python
-- MAGIC 
-- MAGIC df = spark.sql("""
-- MAGIC   SELECT
-- MAGIC     id,
-- MAGIC     rand() random_number,
-- MAGIC     'one one one' str1,
-- MAGIC     'two one one' str2,
-- MAGIC     'three one one' str3
-- MAGIC   FROM
-- MAGIC     RANGE(0, ${test.nrows}, 1, ${test.npartitions})
-- MAGIC """)
-- MAGIC 
-- MAGIC df_indexed = dfZipWithUniqueIndex(df, offset=1, colName="sk")
-- MAGIC df_indexed.createOrReplaceTempView("CHANGE_SET1")
-- MAGIC spark.sql("""INSERT INTO TEST_DIM SELECT * FROM CHANGE_SET1""")

-- COMMAND ----------

-- MAGIC %python
-- MAGIC N = spark.sql("select max(sk) from TEST_DIM").collect()[0][0]
-- MAGIC spark.conf.set("test.max_sk", N)  # set context variable for use in next insert batch
-- MAGIC print(spark.conf.get("test.max_sk"))

-- COMMAND ----------

-- MAGIC %python
-- MAGIC 
-- MAGIC df = spark.sql("""
-- MAGIC   SELECT
-- MAGIC     id,
-- MAGIC     rand() random_number,
-- MAGIC     'one one one' str1,
-- MAGIC     'two one one' str2,
-- MAGIC     'three one one' str3
-- MAGIC   FROM
-- MAGIC     RANGE(0, ${test.nrows}, 1, ${test.npartitions})
-- MAGIC """)
-- MAGIC 
-- MAGIC df_indexed = dfZipWithUniqueIndex(df, offset=N+1, colName="sk")
-- MAGIC df_indexed.createOrReplaceTempView("CHANGE_SET2")
-- MAGIC spark.sql("""INSERT INTO TEST_DIM SELECT * FROM CHANGE_SET2""")

-- COMMAND ----------

SELECT format_number(count(*),0), format_number(count(DISTINCT sk),0) FROM TEST_DIM

-- COMMAND ----------

SELECT min(sk), format_number(max(sk),0) FROM TEST_DIM

-- COMMAND ----------

-- MAGIC %md ## Evaluation of ZipWithUniqueIndex
-- MAGIC * Uniqueness
-- MAGIC * Distribution
-- MAGIC * DBA Perspective
-- MAGIC * Fact & Dimension table partitioning (spark performance)
-- MAGIC * Parallel?
-- MAGIC * Efficient!

-- COMMAND ----------

-- MAGIC %md ## ZipWithIndex vs. ZipWithUniqueIndex
-- MAGIC 
-- MAGIC [Starting with Spark 1.0 there are two methods you can use to solve this easily:](https://stackoverflow.com/questions/23939153/how-to-assign-unique-contiguous-numbers-to-elements-in-a-spark-rdd)
-- MAGIC 
-- MAGIC * RDD.zipWithIndex is just like Seq.zipWithIndex, it adds contiguous (Long) numbers. This needs to count the elements in each partition first, so your input will be evaluated twice. Cache your input RDD if you want to use this.
-- MAGIC * RDD.zipWithUniqueId also gives you unique Long IDs, but they are not guaranteed to be contiguous. (They will only be contiguous if each partition has the same number of elements.) The upside is that this does not need to know anything about the input, so it will not cause double-evaluation.

-- COMMAND ----------

-- MAGIC %md # Test Hash

-- COMMAND ----------

TRUNCATE TABLE TEST_DIM

-- COMMAND ----------

WITH sample_data (
  SELECT
    id,
    rand() random_number,
    'one one one' str1,
    'two one one' str2,
    'three one one' str3
  FROM
    RANGE(0, ${test.nrows}, 1, ${test.npartitions})  -- start, end, step, numPartitions 
)
INSERT INTO TEST_DIM
SELECT hash(concat_ws('||',id, random_number, str1, str2, str3)) as sk, * FROM sample_data

-- COMMAND ----------

WITH sample_data (
  SELECT
    id,
    rand() random_number,
    'one one one' str1,
    'two one one' str2,
    'three one one' str3
  FROM
    RANGE(${test.nrows}+1, ${test.nrows}+${test.nrows}+1, 1, ${test.npartitions})  -- start, end, step, numPartitions 
)
INSERT INTO TEST_DIM
SELECT hash(concat_ws('||',id, random_number, str1, str2, str3)) as sk, * FROM sample_data

-- COMMAND ----------

-- DBTITLE 1,Test for collisions
SELECT format_number(count(*),0), format_number(count(DISTINCT sk),0) FROM TEST_DIM

-- COMMAND ----------

-- MAGIC %md ## Evaluation of hash()
-- MAGIC * Uniqueness
-- MAGIC * DBA Perspective
-- MAGIC * Fact & Dimension table partitioning (spark performance)

-- COMMAND ----------

-- MAGIC %md # Test row_number()

-- COMMAND ----------

CREATE OR REPLACE TABLE TEST_DIM (
 SK BIGINT COMMENT 'SURROGATE KEY',
 ID BIGINT COMMENT 'Identifier',
 random_number DOUBLE COMMENT "Random number",
 str1 STRING COMMENT 'STRING VAL 1',
 str2 STRING COMMENT 'STRING VAL 2',
 str3 STRING COMMENT 'STRING VAL 3'
) USING DELTA
LOCATION 'dbfs:/tmp/test_dim_remove_me'

-- COMMAND ----------

WITH sample_data (
  SELECT
    id,
    rand() random_number,
    'one one one' str1,
    'two one one' str2,
    'three one one' str3
  FROM
    RANGE(0, ${test.nrows}, 1, ${test.npartitions})  -- start, end, step, numPartitions 
)
INSERT INTO TEST_DIM
SELECT row_number() OVER(ORDER BY NULL) as sk, * FROM sample_data

-- COMMAND ----------

WITH sample_data (
  SELECT
    id,
    rand() random_number,
    'one one one' str1,
    'two one one' str2,
    'three one one' str3
  FROM
    RANGE(0, ${test.nrows}, 1, ${test.npartitions})  -- start, end, step, numPartitions 
)
INSERT INTO TEST_DIM
SELECT row_number() OVER(ORDER BY NULL) + (select max(sk) from TEST_DIM) as sk, * FROM sample_data

-- COMMAND ----------

SELECT format_number(count(*),0), format_number(count(DISTINCT sk),0) FROM TEST_DIM

-- COMMAND ----------

SELECT min(sk), max(sk) FROM TEST_DIM

-- COMMAND ----------

select sk from TEST_DIM

-- COMMAND ----------

-- MAGIC %md ## Evaluation of row_number()
-- MAGIC * Uniqueness
-- MAGIC * Distribution
-- MAGIC * DBA Perspective
-- MAGIC * Fact & Dimension table partitioning (spark performance)
-- MAGIC * Parallel execution? https://stackoverflow.com/questions/48500442/spark-dataset-unique-id-performance-row-number-vs-monotonically-increasing-id

-- COMMAND ----------

-- MAGIC %md # Test MD5 hash

-- COMMAND ----------

CREATE OR REPLACE TABLE TEST_DIM (
 SK STRING COMMENT 'SURROGATE KEY',  --- change to string type
 ID BIGINT COMMENT 'Identifier',
 random_number DOUBLE COMMENT "Random number",
 str1 STRING COMMENT 'STRING VAL 1',
 str2 STRING COMMENT 'STRING VAL 2',
 str3 STRING COMMENT 'STRING VAL 3'
) USING DELTA
LOCATION 'dbfs:/tmp/test_dim_remove_me'

-- COMMAND ----------

WITH sample_data (
  SELECT
    id,
    rand() random_number,
    'one one one' str1,
    'two one one' str2,
    'three one one' str3
  FROM
    RANGE(0, ${test.nrows}, 1, ${test.npartitions})  -- start, end, step, numPartitions 
)
INSERT INTO TEST_DIM
SELECT md5(concat_ws('||',id, random_number, str1, str2, str3)) as sk, * FROM sample_data

-- COMMAND ----------

WITH sample_data (
  SELECT
    id,
    rand() random_number,
    'one one one' str1,
    'two one one' str2,
    'three one one' str3
  FROM
    RANGE(${test.nrows}+1, ${test.nrows}+${test.nrows}+1, 1, ${test.npartitions})  -- start, end, step, numPartitions 
)
INSERT INTO TEST_DIM
SELECT md5(concat_ws('||',id, random_number, str1, str2, str3)) as sk, * FROM sample_data

-- COMMAND ----------

-- DBTITLE 1,Test for collisions
SELECT format_number(count(*),0), format_number(count(DISTINCT sk),0) FROM TEST_DIM

-- COMMAND ----------

SELECT SK from TEST_DIM

-- COMMAND ----------

-- MAGIC %md ## Evaluation of md5()
-- MAGIC * Uniqueness
-- MAGIC * Distribution
-- MAGIC * DBA Perspective
-- MAGIC * Fact & Dimension table partitioning (spark performance)
-- MAGIC * Parallel

-- COMMAND ----------

-- MAGIC %md # Wrap Up
-- MAGIC * monotonically_increasing_id()
-- MAGIC * zipWithIndex() [Vote](https://issues.apache.org/jira/browse/SPARK-23074)
-- MAGIC * zipWithUniqueIndex()
-- MAGIC * row_number()
-- MAGIC * hash()
-- MAGIC * md5()

-- COMMAND ----------

