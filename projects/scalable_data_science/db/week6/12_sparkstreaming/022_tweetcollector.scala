// Databricks notebook source exported at Fri, 24 Jun 2016 23:58:12 UTC
// MAGIC %md
// MAGIC 
// MAGIC # [Scalable Data Science](http://www.math.canterbury.ac.nz/~r.sainudiin/courses/ScalableDataScience/)
// MAGIC 
// MAGIC 
// MAGIC ### prepared by [Raazesh Sainudiin](https://nz.linkedin.com/in/raazesh-sainudiin-45955845) and [Sivanand Sivaram](https://www.linkedin.com/in/sivanand)
// MAGIC 
// MAGIC *supported by* [![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/databricks_logoTM_200px.png)](https://databricks.com/)
// MAGIC and 
// MAGIC [![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/AWS_logoTM_200px.png)](https://www.awseducate.com/microsite/CommunitiesEngageHome)

// COMMAND ----------

// MAGIC %md
// MAGIC The [html source url](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/db/week6/12_SparkStreaming/022_TweetCollector.html) of this databricks notebook and its recorded Uji ![Image of Uji, Dogen's Time-Being](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/UjiTimeBeingDogen.png "uji"):
// MAGIC 
// MAGIC [![sds/uji/week6/12_SparkStreaming/022_TweetCollector](http://img.youtube.com/vi/jqLcr2eS-Vs/0.jpg)](https://www.youtube.com/v/jqLcr2eS-Vs?rel=0&autoplay=1&modestbranding=1&start=2112&end=3535)

// COMMAND ----------

// MAGIC %md
// MAGIC # Tweet Collector - capture live tweets
// MAGIC 
// MAGIC ### First let's take the twitter stream and write to DBFS as json files
// MAGIC 
// MAGIC #### See the notebook 022_TweetGenericCollector (this notebook is not robust and it is only for demo)!!!

// COMMAND ----------

import org.apache.spark._
import org.apache.spark.storage._
import org.apache.spark.streaming._
import org.apache.spark.streaming.twitter.TwitterUtils

import twitter4j.auth.OAuthAuthorization
import twitter4j.conf.ConfigurationBuilder

// COMMAND ----------

// MAGIC %md
// MAGIC Let's create a directory in dbfs for storing tweets in the cluster's distributed file system.

// COMMAND ----------

val rawTweetsDirectory="/rawTweets"

// COMMAND ----------

dbutils.fs.rm(rawTweetsDirectory, true) // to remove a pre-existing directory and start from scratch uncomment and evaluate this cell

// COMMAND ----------

// MAGIC %md
// MAGIC Capture tweets in every sliding window of `slideInterval` many milliseconds.

// COMMAND ----------

val slideInterval = new Duration(1 * 1000) // 1 * 1000 = 1000 milli-seconds = 1 sec

// COMMAND ----------

// MAGIC %md
// MAGIC Recall that **Discretized Stream** or **DStream** is the basic abstraction provided
// MAGIC by Spark Streaming. It represents a continuous stream of data, either
// MAGIC the input data stream received from source, or the processed data stream
// MAGIC generated by transforming the input stream. Internally, a DStream is
// MAGIC represented by a continuous series of RDDs, which is Spark?s abstraction
// MAGIC of an immutable, distributed dataset (see [Spark Programming
// MAGIC Guide](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds)
// MAGIC for more details). Each RDD in a DStream contains data from a certain
// MAGIC interval, as shown in the following figure.
// MAGIC 
// MAGIC ![Spark
// MAGIC Streaming](http://spark.apache.org/docs/latest/img/streaming-dstream.png "Spark Streaming data flow")

// COMMAND ----------

// MAGIC %md
// MAGIC Let's import googles json library next.

// COMMAND ----------

import com.google.gson.Gson // the Library has already been attached to this cluster (show live how to do this from scratch?)

// COMMAND ----------

// MAGIC %md
// MAGIC Our goal is to take each RDD in the twitter DStream and write it as a json file in our dbfs.

// COMMAND ----------

// Create a Spark Streaming Context.
val ssc = new StreamingContext(sc, slideInterval)

// COMMAND ----------

// MAGIC %md
// MAGIC 
// MAGIC CAUTION: Extracting knowledge from tweets is "easy" using techniques shown here, but one has to take responsibility for the use of this knowledge and conform to the rules and policies linked below.
// MAGIC 
// MAGIC Remeber that the use of twitter itself comes with various strings attached. Read:
// MAGIC 
// MAGIC - [Twitter Rules](https://twitter.com/rules)
// MAGIC 
// MAGIC 
// MAGIC Crucially, the use of the content from twitter by you (as done in this worksheet) comes with some strings.  Read:
// MAGIC - [Developer Agreement & Policy Twitter Developer Agreement](https://dev.twitter.com/overview/terms/agreement-and-policy)

// COMMAND ----------

// MAGIC %md
// MAGIC 
// MAGIC ### Enter your own Twitter API Credentials.
// MAGIC * Go to https://apps.twitter.com and look up your Twitter API Credentials, or create an app to create them.
// MAGIC * Run this cell for the input cells to appear.
// MAGIC * Enter your credentials.
// MAGIC * Run the cell again to pick up your defaults.
// MAGIC 
// MAGIC The cell-below is hidden to not expose the Twitter API Credentials: `consumerKey`, `consumerSecret`, `accessToken` and `accessTokenSecret`.

// COMMAND ----------

System.setProperty("twitter4j.oauth.consumerKey", getArgument("1. Consumer Key (API Key)", ""))
System.setProperty("twitter4j.oauth.consumerSecret", getArgument("2. Consumer Secret (API Secret)", ""))
System.setProperty("twitter4j.oauth.accessToken", getArgument("3. Access Token", ""))
System.setProperty("twitter4j.oauth.accessTokenSecret", getArgument("4. Access Token Secret", ""))

// COMMAND ----------

// MAGIC %md
// MAGIC If you see warnings then ignore for now:
// MAGIC [https://forums.databricks.com/questions/6941/change-in-getargument-for-notebook-input.html](https://forums.databricks.com/questions/6941/change-in-getargument-for-notebook-input.html).

// COMMAND ----------

// Create a Twitter Stream for the input source. 
val auth = Some(new OAuthAuthorization(new ConfigurationBuilder().build()))
val twitterStream = TwitterUtils.createStream(ssc, auth)

// COMMAND ----------

// MAGIC %md
// MAGIC Let's map the tweets into json formatted string (one tweet per line).

// COMMAND ----------

val twitterStreamJson = twitterStream.map(x => { val gson = new Gson();
                                                 val xJson = gson.toJson(x)
                                                 xJson
                                               }) 

// COMMAND ----------

var numTweetsCollected = 0L // track number of tweets collected
val partitionsEachInterval = 1 // This tells the number of partitions in each RDD of tweets in the DStream.

twitterStreamJson.foreachRDD((rdd, time) => { // for each RDD in the DStream
      val count = rdd.count()
      if (count > 0) {
        val outputRDD = rdd.repartition(partitionsEachInterval) // repartition as desired
        outputRDD.saveAsTextFile(rawTweetsDirectory + "/tweets_" + time.milliseconds.toString) // save as textfile
        numTweetsCollected += count // update with the latest count
      }
  })

// COMMAND ----------

// MAGIC %md 
// MAGIC Nothing has actually happened yet.
// MAGIC 
// MAGIC Let's start the spark streaming context we have created next.

// COMMAND ----------

ssc.start()

// COMMAND ----------

// MAGIC %md
// MAGIC Let's look at the spark UI now and monitor the streaming job in action!  Go to `Clusters` on the left and click on `UI` and then `Streaming`.

// COMMAND ----------

numTweetsCollected // number of tweets collected so far

// COMMAND ----------

// MAGIC %md
// MAGIC Note that you could easilt fill up disk space!!!
// MAGIC 
// MAGIC So let's stop the streaming job next.

// COMMAND ----------

ssc.stop(stopSparkContext = false) // gotto stop soon!!!

// COMMAND ----------

// MAGIC %md
// MAGIC Let's make sure that the `Streaming` UI is not active in the `Clusters` `UI`.

// COMMAND ----------

StreamingContext.getActive.foreach { _.stop(stopSparkContext = false) } // extra cautious stopping of all active streaming contexts

// COMMAND ----------

// MAGIC %md
// MAGIC ## Let's examine what was saved in dbfs

// COMMAND ----------

display(dbutils.fs.ls("/rawTweets/"))

// COMMAND ----------

val tweetsDir = "/rawTweets/tweets_1459392400000/" // use an existing file, may have to rename folder based on output above!

// COMMAND ----------

display(dbutils.fs.ls(tweetsDir)) 

// COMMAND ----------

sc.textFile(tweetsDir+"part-00000").count()

// COMMAND ----------

val outJson = sqlContext.read.json(tweetsDir+"part-00000")

// COMMAND ----------

outJson.printSchema()

// COMMAND ----------

outJson.select("id","text").show(false)

// COMMAND ----------

// MAGIC %md
// MAGIC Clearly there is a lot one can do with tweets!

// COMMAND ----------

// MAGIC %md
// MAGIC ## Next, let's write the tweets into a scalable commercial cloud storage system
// MAGIC 
// MAGIC We will make sure to write the tweets to AWS's simple storage service or S3, a scalable storage system in the cloud. See [https://aws.amazon.com/s3/](https://aws.amazon.com/s3/).

// COMMAND ----------

// Replace with your AWS S3 credentials
//
// NOTE: Set the access to this notebook appropriately to protect the security of your keys.
// Or you can delete this cell after you run the mount command below once successfully.

val AccessKey = getArgument("1. ACCESS_KEY", "REPLACE_WITH_YOUR_ACCESS_KEY")
val SecretKey = getArgument("2. SECRET_KEY", "REPLACE_WITH_YOUR_SECRET_KEY")
val EncodedSecretKey = SecretKey.replace("/", "%2F")
val AwsBucketName = getArgument("3. S3_BUCKET", "REPLACE_WITH_YOUR_S3_BUCKET")
val MountName = getArgument("4. MNT_NAME", "REPLACE_WITH_YOUR_MOUNT_NAME")
val s3Filename = "tweetDump"

// COMMAND ----------

// MAGIC %md
// MAGIC Now just mount s3 as follows:

// COMMAND ----------

dbutils.fs.mount(s"s3a://$AccessKey:$EncodedSecretKey@$AwsBucketName", s"/mnt/$MountName")

// COMMAND ----------

// MAGIC %md
// MAGIC Now you can use the `dbutils` commands freely to access data in S3.

// COMMAND ----------

dbutils.fs.help()

// COMMAND ----------

display(dbutils.fs.ls(s"/mnt/")) // list the files in s3

// COMMAND ----------

dbutils.fs.cp("dbfs:/rawTweets",s"/mnt/$MountName/rawTweetsInS3/",recurse=true) // copy all the tweets

// COMMAND ----------

display(dbutils.fs.ls(s"/mnt/$MountName/rawTweetsInS3")) // list the files copied into s3

// COMMAND ----------

dbutils.fs.rm(s"/mnt/$MountName/rawTweetsInS3",recurse=true) // remove all the files from s3

// COMMAND ----------

display(dbutils.fs.ls("/mnt/")) // list the files in s3

// COMMAND ----------

//display(dbutils.fs.ls(s"/mnt/$MountName/rawTweetsInS3")) // it has been removed

// COMMAND ----------

dbutils.fs.unmount(s"/mnt/$MountName") // finally unmount when done

// COMMAND ----------

// MAGIC %md
// MAGIC 
// MAGIC # [Scalable Data Science](http://www.math.canterbury.ac.nz/~r.sainudiin/courses/ScalableDataScience/)
// MAGIC 
// MAGIC 
// MAGIC ### prepared by [Raazesh Sainudiin](https://nz.linkedin.com/in/raazesh-sainudiin-45955845) and [Sivanand Sivaram](https://www.linkedin.com/in/sivanand)
// MAGIC 
// MAGIC *supported by* [![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/databricks_logoTM_200px.png)](https://databricks.com/)
// MAGIC and 
// MAGIC [![](https://raw.githubusercontent.com/raazesh-sainudiin/scalable-data-science/master/images/AWS_logoTM_200px.png)](https://www.awseducate.com/microsite/CommunitiesEngageHome)