{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplify Data Lake Reliability with Delta Lake and Python, SQL Utilities, and In-Place Migration\n",
    "\n",
    "We are excited to announce the release of Delta Lake 0.4.0 which introduces Python APIs for manipulating and managing data in Delta tables. The key features in this release are:\n",
    "\n",
    "* **Python APIs for DML and utility operations** ([#89](https://github.com/delta-io/delta/issues/89)) - You can now use Python APIs to update/delete/merge data in Delta Lake tables and to run utility operations (i.e., vacuum, history) on them. These are great for building complex workloads in Python, e.g., [Slowly Changing Dimension (SCD)](https://docs.delta.io/0.4.0/delta-update.html#slowly-changing-data-scd-type-2-operation-into-delta-tables) operations, merging [change data](https://docs.delta.io/0.4.0/delta-update.html#write-change-data-into-a-delta-table) for replication, and [upserts from streaming queries](https://docs.delta.io/0.4.0/delta-update.html#upsert-from-streaming-queries-using-foreachbatch). See the [documentation](https://docs.delta.io/0.4.0/delta-update.html) for more details.\n",
    "\n",
    "* **Convert-to-Delta** ([#78](https://github.com/delta-io/delta/issues/78)) - You can now convert a Parquet table in place to a Delta Lake table without rewriting any of the data. This is great for converting very large Parquet tables which would be costly to rewrite as a Delta table. Furthermore, this process is reversible - you can convert a Parquet table to Delta Lake table, operate on it (e.g., delete or merge), and easily convert it back to a Parquet table. See the [documentation](https://docs.delta.io/0.4.0/delta-utility.html#convert-to-delta) for more details.\n",
    "\n",
    "* **SQL for utility operations** - You can now use SQL to run utility operations vacuum and history. See the [documentation](https://docs.delta.io/0.4.0/delta-utility.html#enable-sql-commands-within-apache-spark) for more details on how to configure Spark to execute these Delta-specific SQL commands.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "Configure locations for the source file and where the Delta Lake Table will be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tripdelaysFilePath = \"/usr/local/Cellar/spark/data/departuredelays.csv\"\n",
    "pathToEventsTable = \"/usr/local/Cellar/spark/spark-2.4.3-bin-hadoop2.7/departureDelays.delta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `departureDelays` DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "departureDelays = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(tripdelaysFilePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save table as Delta Lake (update `pathToEventsTable` to match the following location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "departureDelays.write.format(\"delta\").mode(\"overwrite\").save(\"departureDelays.delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Delta Lake table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_delta = spark.read.format(\"delta\").load(\"departureDelays.delta\")\n",
    "delays_delta.createOrReplaceTempView(\"delays_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get count of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(1)\n",
       "0      1698"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from delays_delta where origin = 'SEA' and destination = 'SFO'\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review File System**: Note there are four files initially created as part of the table creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m_delta_log\u001b[m\u001b[m/\r\n",
      "part-00000-091cff02-ca22-473d-bb22-13164be0846b-c000.snappy.parquet\r\n",
      "part-00001-e61a8edb-210a-4692-a45e-388caf74dd62-c000.snappy.parquet\r\n",
      "part-00002-9c17ccd7-7c13-429e-b26c-eabad1e1d58a-c000.snappy.parquet\r\n",
      "part-00003-bf58ae04-4900-425c-bdd4-c29aa8c454e4-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "%ls $pathToEventsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deletes\n",
    "With Delta Lake, you can delete data with the Python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "deltaTable = DeltaTable.forPath(spark, pathToEventsTable)\n",
    "deltaTable.delete(\"delay < 0\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(1)\n",
       "0       837"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Row Count\n",
    "spark.sql(\"select count(1) from delays_delta where origin = 'SEA' and destination = 'SFO'\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review File System**: Note that while we deleted early (and on-time) flights, there are now eight files (instead of the four files initially created as part of the table creation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m_delta_log\u001b[m\u001b[m/\r\n",
      "part-00000-091cff02-ca22-473d-bb22-13164be0846b-c000.snappy.parquet\r\n",
      "part-00000-fb071d3b-9d25-4faf-a04f-26d35716315b-c000.snappy.parquet\r\n",
      "part-00001-8db427ad-edd5-4c99-8578-01da4b4ea921-c000.snappy.parquet\r\n",
      "part-00001-e61a8edb-210a-4692-a45e-388caf74dd62-c000.snappy.parquet\r\n",
      "part-00002-8c19b041-8e4b-4c38-be54-3d32bce76f83-c000.snappy.parquet\r\n",
      "part-00002-9c17ccd7-7c13-429e-b26c-eabad1e1d58a-c000.snappy.parquet\r\n",
      "part-00003-bf58ae04-4900-425c-bdd4-c29aa8c454e4-c000.snappy.parquet\r\n",
      "part-00003-c08cd37e-659b-49c8-9ab1-5a39862dddc9-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "%ls $pathToEventsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updates\n",
    "Update flights originating from Detroit (DTW) to now be from Seattle (SEA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.update(\"origin = 'DTW'\", { \"origin\": \"'SEA'\" } ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count(1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count(1)\n",
       "0       986"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(1) from delays_delta where origin = 'SEA' and destination = 'SFO'\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View History\n",
    "View the table history (note the create table, insert, and update operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>userId</th>\n",
       "      <th>userName</th>\n",
       "      <th>operation</th>\n",
       "      <th>operationParameters</th>\n",
       "      <th>job</th>\n",
       "      <th>notebook</th>\n",
       "      <th>clusterId</th>\n",
       "      <th>readVersion</th>\n",
       "      <th>isolationLevel</th>\n",
       "      <th>isBlindAppend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-10-02 17:56:46</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>UPDATE</td>\n",
       "      <td>{'predicate': '(origin#767 = DTW)'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-10-02 17:56:38</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>DELETE</td>\n",
       "      <td>{'predicate': '[\"(`delay` &lt; 0)\"]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-10-02 17:56:25</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>{'mode': 'Overwrite', 'partitionBy': '[]'}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   version           timestamp userId userName operation  \\\n",
       "0        2 2019-10-02 17:56:46   None     None    UPDATE   \n",
       "1        1 2019-10-02 17:56:38   None     None    DELETE   \n",
       "2        0 2019-10-02 17:56:25   None     None     WRITE   \n",
       "\n",
       "                          operationParameters   job notebook clusterId  \\\n",
       "0         {'predicate': '(origin#767 = DTW)'}  None     None      None   \n",
       "1          {'predicate': '[\"(`delay` < 0)\"]'}  None     None      None   \n",
       "2  {'mode': 'Overwrite', 'partitionBy': '[]'}  None     None      None   \n",
       "\n",
       "   readVersion isolationLevel  isBlindAppend  \n",
       "0          1.0           None          False  \n",
       "1          0.0           None          False  \n",
       "2          NaN           None          False  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.history().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate counts for each version of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEA -> SFO Counts: Create Table: 1698, Delete: 837, Update: 986\n"
     ]
    }
   ],
   "source": [
    "dfv0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"departureDelays.delta\")\n",
    "dfv1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(\"departureDelays.delta\")\n",
    "dfv2 = spark.read.format(\"delta\").option(\"versionAsOf\", 2).load(\"departureDelays.delta\")\n",
    "\n",
    "cnt0 = dfv0.where(\"origin = 'SEA'\").where(\"destination = 'SFO'\").count()\n",
    "cnt1 = dfv1.where(\"origin = 'SEA'\").where(\"destination = 'SFO'\").count()\n",
    "cnt2 = dfv2.where(\"origin = 'SEA'\").where(\"destination = 'SFO'\").count()\n",
    "\n",
    "print(\"SEA -> SFO Counts: Create Table: %s, Delete: %s, Update: %s\" % (cnt0, cnt1, cnt2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review File System**: Note the number of files based on the preceding operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m_delta_log\u001b[m\u001b[m/\r\n",
      "part-00000-091cff02-ca22-473d-bb22-13164be0846b-c000.snappy.parquet\r\n",
      "part-00000-fa942ead-0f90-4a99-8949-510d00a77597-c000.snappy.parquet\r\n",
      "part-00000-fb071d3b-9d25-4faf-a04f-26d35716315b-c000.snappy.parquet\r\n",
      "part-00001-8db427ad-edd5-4c99-8578-01da4b4ea921-c000.snappy.parquet\r\n",
      "part-00001-e2639517-7fea-4bc0-ba74-3af982025112-c000.snappy.parquet\r\n",
      "part-00001-e61a8edb-210a-4692-a45e-388caf74dd62-c000.snappy.parquet\r\n",
      "part-00002-8c19b041-8e4b-4c38-be54-3d32bce76f83-c000.snappy.parquet\r\n",
      "part-00002-9c17ccd7-7c13-429e-b26c-eabad1e1d58a-c000.snappy.parquet\r\n",
      "part-00002-c85d41c2-0ea1-451a-9bbf-52e7bf87d569-c000.snappy.parquet\r\n",
      "part-00003-bf58ae04-4900-425c-bdd4-c29aa8c454e4-c000.snappy.parquet\r\n",
      "part-00003-c08cd37e-659b-49c8-9ab1-5a39862dddc9-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "%ls $pathToEventsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vacuum\n",
    "Remove older data (by default 7 days) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltaTable.vacuum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m_delta_log\u001b[m\u001b[m/\r\n",
      "part-00000-fa942ead-0f90-4a99-8949-510d00a77597-c000.snappy.parquet\r\n",
      "part-00001-e2639517-7fea-4bc0-ba74-3af982025112-c000.snappy.parquet\r\n",
      "part-00002-c85d41c2-0ea1-451a-9bbf-52e7bf87d569-c000.snappy.parquet\r\n",
      "part-00003-c08cd37e-659b-49c8-9ab1-5a39862dddc9-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "%ls $pathToEventsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's not forget, Delta Lake 0.4.0 also includes `MERGE` in the Python API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge\n",
    "Let's merge another table with the `departureDelays` table with [data deduplication](https://docs.delta.io/0.4.0/delta-update.html#data-deduplication-when-writing-into-delta-tables).  Let's start by viewing data that will be impacted by the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>delay</th>\n",
       "      <th>distance</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1010521</td>\n",
       "      <td>0</td>\n",
       "      <td>590</td>\n",
       "      <td>SEA</td>\n",
       "      <td>SFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1010710</td>\n",
       "      <td>31</td>\n",
       "      <td>590</td>\n",
       "      <td>SEA</td>\n",
       "      <td>SFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1010730</td>\n",
       "      <td>5</td>\n",
       "      <td>590</td>\n",
       "      <td>SEA</td>\n",
       "      <td>SFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1010955</td>\n",
       "      <td>104</td>\n",
       "      <td>590</td>\n",
       "      <td>SEA</td>\n",
       "      <td>SFO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date  delay  distance origin destination\n",
       "0  1010521      0       590    SEA         SFO\n",
       "1  1010710     31       590    SEA         SFO\n",
       "2  1010730      5       590    SEA         SFO\n",
       "3  1010955    104       590    SEA         SFO"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from delays_delta where origin = 'SEA' and destination = 'SFO' and date like '1010%' order by date limit 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create our `merge_table` which contains three rows:\n",
    "* 1010710: this row is a duplicate\n",
    "* 1010521: this row will be updated with a new delay value\n",
    "* 1010822: this is a new row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>delay</th>\n",
       "      <th>distance</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1010521</td>\n",
       "      <td>10</td>\n",
       "      <td>590</td>\n",
       "      <td>SEA</td>\n",
       "      <td>SFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1010710</td>\n",
       "      <td>31</td>\n",
       "      <td>590</td>\n",
       "      <td>SEA</td>\n",
       "      <td>SFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1010832</td>\n",
       "      <td>31</td>\n",
       "      <td>590</td>\n",
       "      <td>SEA</td>\n",
       "      <td>SFO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date  delay  distance origin destination\n",
       "0  1010521     10       590    SEA         SFO\n",
       "1  1010710     31       590    SEA         SFO\n",
       "2  1010832     31       590    SEA         SFO"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = [(1010521, 10, 590, 'SEA', 'SFO'), (1010710, 31, 590, 'SEA', 'SFO'), (1010832, 31, 590, 'SEA', 'SFO')]\n",
    "cols = ['date', 'delay', 'distance', 'origin', 'destination']\n",
    "merge_table = spark.createDataFrame(items, cols)\n",
    "merge_table.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our merge statement that will handle the duplicates, updates, and add a new row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.alias(\"flights\") \\\n",
    "    .merge(merge_table.alias(\"updates\"),\"flights.date = updates.date\") \\\n",
    "    .whenMatchedUpdate(set = { \"delay\" : \"updates.delay\" } ) \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>delay</th>\n",
       "      <th>distance</th>\n",
       "      <th>origin</th>\n",
       "      <th>destination</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1010521</td>\n",
       "      <td>10</td>\n",
       "      <td>590</td>\n",
       "      <td>SEA</td>\n",
       "      <td>SFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1010710</td>\n",
       "      <td>31</td>\n",
       "      <td>590</td>\n",
       "      <td>SEA</td>\n",
       "      <td>SFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1010730</td>\n",
       "      <td>5</td>\n",
       "      <td>590</td>\n",
       "      <td>SEA</td>\n",
       "      <td>SFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1010832</td>\n",
       "      <td>31</td>\n",
       "      <td>590</td>\n",
       "      <td>SEA</td>\n",
       "      <td>SFO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1010955</td>\n",
       "      <td>104</td>\n",
       "      <td>590</td>\n",
       "      <td>SEA</td>\n",
       "      <td>SFO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date  delay  distance origin destination\n",
       "0  1010521     10       590    SEA         SFO\n",
       "1  1010710     31       590    SEA         SFO\n",
       "2  1010730      5       590    SEA         SFO\n",
       "3  1010832     31       590    SEA         SFO\n",
       "4  1010955    104       590    SEA         SFO"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from delays_delta where origin = 'SEA' and destination = 'SFO' and date like '1010%' order by date limit 10\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the previous cells, notice the following:\n",
    "* There is only one row for the date `1010710` as `merge` automatically takes care of **data deduplication**\n",
    "* The row for the date `1010521` has the `delay` value **updated** from 0 to 10.\n",
    "* The row for the date `1010821` has been added as this date did not exist, hence it was **inserted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
