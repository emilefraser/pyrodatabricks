{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"15c404f9-91b1-4d65-ab5a-468d3bb044f9","showTitle":false,"title":""}},"source":["Functions available from **DataLakeHelperFunctions**\n","\n","|Function|Purpose|\n","|--|--|\n","|mount_lake_container|Takes a container name and mounts it to Databricks. Prints out the name of the mount point. Uses Service Principle Auth. |\n","|convert_full_file_path_to_mount_point|Takes a parameter that is a full file path to a file in a Azure Data Lake. Converts the string to use the Databricks mount point to the applicbale container instead.|\n","|get_latest_modified_file_from_directory|For a given directory, return the file that was last modified|\n","| convert_container_and_directory_to_mountpoint|Takes the given data lake container name and directory inside the container and returns a path inside the mount point using standard formatting.|\n","|get_json_df_tangent|Converts a column in a PySpark DataFrame from string to Struct|\n","|execute_autoflatten|Flattens a JSON column in a PySpark DataFrame|\n","|execute_autoflatten_json|Receives a Data Frame with a column that contains JSON code as strings, converts it to typed JSON and flattens it. |\n","\n","Classes available from **DataLakeHelperFunctions**\n","|Class|Purpose|\n","|--|--|\n","|Log4jWrapper|A class that creates a callable instance of the PySpark built in Log4j JVM logging object. Log4j is a Java-based logging utility part of the Apache Logging Services.|"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b728844a-9487-4594-88dd-9d0515a7fd40","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def mount_lake_container(pAdlsContainerName, pSecretScopeName='KeyVault',  pStorageAccountName='DataLakeStorageAccountName', pSecretClientID='DataLakeAuthServicePrincipleClientID', pSecretClientSecret='DataLakeAuthServicePrincipleClientSecret', pSecretTenantID='DataLakeAuthServicePrincipleTenantID'):\n","    \"\"\"\n","    mount_lake_container:\n","        Takes a container name and mounts it to Databricks for easy access. \n","        Prints out the name of the mount point. \n","        Uses a service princple to authenticate.\n","      \n","    param pAdlsContainerName: The name of the container that will be mounted.\n","    param pSecretScopeName: Databricks Secret Scope name.\n","    param pStorageAccountName: The storage accounts name.\n","    param pSecretClientID: Name of the key vault secret containing the Service Principle name.\n","    param pSecretClientSecret: Name of the key vault secret containing the Service Principle Password.\n","    param pSecretTenantID: Name of the key vault secret containing the Tenant ID\n","    \"\"\"\n","    # Define the variables used for creating connection strings - Data Lake Related\n","    vAdlsAccountName = dbutils.secrets.get(scope=pSecretScopeName, key=pStorageAccountName) # e.g. \"dummydatalake\" - the storage account name itself\n","    vAdlsContainerName = pAdlsContainerName # e.g. rawdata, bronze, silver, gold, platinum etc.\n","    vMountPoint = \"/mnt/datalake_\" + vAdlsContainerName #fixed since we already parameterised the container name. Ensures there is a standard in mount point naming\n","\n","    # Get the actual secrets from key vault for the service principle\n","    vApplicationId = dbutils.secrets.get(scope=pSecretScopeName, key=pSecretClientID) # Application (Client) ID\n","    vAuthenticationKey = dbutils.secrets.get(scope=pSecretScopeName, key=pSecretClientSecret) # Application (Client) Secret Key\n","    vTenantId = dbutils.secrets.get(scope=pSecretScopeName, key=pSecretTenantID) # Directory (Tenant) ID\n","\n","    # Using the secrets above, generate the URL to the storage account and the authentication endpoint for OAuth\n","    vEndpoint = \"https://login.microsoftonline.com/\" + vTenantId + \"/oauth2/token\" #Fixed URL for the endpoint\n","    vSource = \"abfss://\" + vAdlsContainerName + \"@\" + vAdlsAccountName + \".dfs.core.windows.net/\"\n","\n","    # Connecting using Service Principal secrets and OAuth\n","    vConfigs = {\"fs.azure.account.auth.type\": \"OAuth\", #standard\n","               \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\", #standard\n","               \"fs.azure.account.oauth2.client.id\": vApplicationId,\n","               \"fs.azure.account.oauth2.client.secret\": vAuthenticationKey,\n","               \"fs.azure.account.oauth2.client.endpoint\": vEndpoint}\n","\n","    # Mount Data Lake Storage to Databricks File System only if the container is not already mounted\n","    # First generate a list of all mount points available already via dbutils.fs.mounts()\n","    # Then it checks the list for the new mount point we are trying to generate.\n","    if not any(mount.mountPoint == vMountPoint for mount in dbutils.fs.mounts()): \n","      dbutils.fs.mount(\n","        source = vSource,\n","        mount_point = vMountPoint,\n","        extra_configs = vConfigs)\n","\n","    # print the mount point used for troubleshooting in the consuming notebook\n","    print(\"Mount Point: \" + vMountPoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"fa6806a3-bcbb-45cd-b991-c4afa4a9d51b","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def convert_full_file_path_to_mount_point(pFullFilePath):\n","    \n","    \"\"\"\n","    convert_full_file_path_to_mount_point:\n","        Takes a parameter that is a full file path to a file in a Azure Data Lake. \n","        Converts the string to use the Databricks mount point to the applicbale container instead.\n","        Format of URL expected: https://dianrandddatalake.blob.core.windows.net/rawdata/DummyAutomatedDirectory/2022/02/28/16/30/wwi-dimstockitem.csv\n","    \"\"\"\n","    \n","    # KeyVault Secret Scope Name - use a variable because it is referenced multiple times\n","    vSecretScopeName = \"KeyVault\" # Fixed standardised name. To ensure deployment from DEV to PROD is seemless. \n","    \n","    # Get the storage account name from key vault for use later when altering the full file path received to reference the data lake container mount point instead\n","    vStorageAccountName = dbutils.secrets.get(scope=vSecretScopeName,key=\"DataLakeStorageAccountName\")\n","    \n","    # String to replace in full file path to have it use the mount point instead\n","    vStorageAccountURL = \"https://\" + vStorageAccountName + \".blob.core.windows.net/\"\n","    \n","    # Value to replace the storage account url with\n","    # Note, this does not contain the suffix with the container name, we get that from the full file path already.\n","    vStorageAccount_MountPointPrefix = \"/mnt/datalake_\"\n","\n","    # Remove the URL of the storage account and replace with the mount point string\n","    # This variable \"pFileFullPath\" can now be used to read the applicable file into memory into a dataframe\n","    pMountPointPath = pFullFilePath.replace(vStorageAccountURL, vStorageAccount_MountPointPrefix)\n","\n","    return pMountPointPath"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"75ca71b9-4127-496a-b6ef-5459cad10d25","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["import os\n","from datetime import datetime\n","\n","def get_dir_content(pPath):\n","    \"\"\"\n","    get_dir_content:\n","        For a folder in the data lake, get the list of files it contains, including all subfolders. \n","        Return Full File Name as well as Last Modified Date time as a generator object. \n","        Output requires conversion into list for consumption.\n","    \"\"\"\n","    #This for loop will check all directories and files inside the provided path\n","    #For each file it contains, return a 2-D array witht he file name and the last modified date time\n","    #The consuming code will need to convert the generater object this returns to a list to consume it\n","    #THe yield function is used to ensure the entire directory contents is scanned. If you used return it would stop after the first object encountered. \n","    for dir_path in dbutils.fs.ls(pPath):\n","        if dir_path.isFile():\n","            #os.stat gets statistics on a path. st_mtime gets the most recent content modification date time\n","            yield [dir_path.path, datetime.fromtimestamp(os.stat('/' + dir_path.path.replace(':','')).st_mtime)]\n","        elif dir_path.isDir() and pPath != dir_path.path:\n","            #if the path is a directory, call the function on it again to check its contents\n","            yield from get_dir_content(dir_path.path)\n","\n","def get_latest_modified_file_from_directory(pDirectory):\n","    \"\"\"\n","    get_latest_modified_file_from_directory:\n","        For a given path to a directory in the data lake, return the file that was last modified. \n","        Uses the get_dir_content function as well.\n","        Input path format expectation: '/mnt/datalake_rawdata'\n","            You can add sub directories as well, as long as you use a registered mount point\n","        Performance: With 588 files, it returns in less than 10 seconds on the lowest cluster size. \n","    \"\"\"\n","    #Call get_dir_content to get a list of all files in this directory and the last modified date tiem of each\n","    vDirectoryContentsList =list(get_dir_content(pDirectory))\n","\n","    #Convert the list returned from get_dir_content into a dataframe so we can manipulate the data easily. Provide it with column headings. \n","    df = spark.createDataFrame(vDirectoryContentsList,['FullFilePath', 'LastModifiedDateTime'])\n","\n","    #Get the latest modified date time scalar value\n","    maxLatestModifiedDateTime = df.agg({\"LastModifiedDateTime\": \"max\"}).collect()[0][0]\n","\n","    #Filter the data frame to the record with the latest modified date time value retrieved\n","    df_filtered = df.filter(df.LastModifiedDateTime == maxLatestModifiedDateTime)\n","    \n","    #return the file name that was last modifed in the given directory\n","    return df_filtered.first()['FullFilePath']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"aca4d544-2d1a-4e5c-b795-1e3f804e193f","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def convert_container_and_directory_to_mountpoint(pContainer, pDirectory):\n","    \"\"\"\n","    convert_container_and_directory_to_mountpoint:\n","        Takes the given data lake container name and directory inside the container and returns a path inside the mount point using standard formatting. \n","    \"\"\"\n","    return \"/mnt/datalake_\" + pContainer + '/' + pDirectory"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5a9089b7-7b02-44a0-97be-7068b48ea26e","showTitle":false,"title":""}},"outputs":[],"source":["def get_json_df(inputDF, json_column_name, spark_session):\n","    '''\n","    Description:\n","    This function provides the schema of json records and the dataframe to be used for flattening. If this doesnt happen, the source JSON String remains a string and cant be queries like JSON\n","        :param inputDF: [type: pyspark.sql.dataframe.DataFrame] input dataframe\n","        :param json_column_name: [type: string] name of the column with json string\n","        :param spark_session: SparkSession object\n","        :return df: dataframe to be used for flattening\n","    '''\n","    # creating a column transformedJSON to create an outer struct\n","    df1 = inputDF.withColumn('transformed_json', concat(lit(\"\"\"{\"transformed_json\" :\"\"\"), inputDF[json_column_name], lit(\"\"\"}\"\"\")))\n","    json_df = spark_session.read.json(df1.rdd.map(lambda row: row.transformed_json))\n","    # get schema\n","    json_schema = json_df.schema\n","    \n","    #Return a dataframe with the orignal column name but with proper JSON typed data\n","    df = df1.drop(json_column_name)\\\n","        .withColumn(json_column_name, from_json(col('transformed_json'), json_schema))\\\n","        .drop('transformed_json')\\\n","        .select(f'{json_column_name}.*', '*')\\\n","        .drop(json_column_name)\\\n","        .withColumnRenamed(\"transformed_json\", json_column_name)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ffc7fe33-3c1a-477e-8e1d-1904d1e509cd","showTitle":false,"title":""}},"outputs":[],"source":["def execute_autoflatten(df, json_column_name):\n","    '''\n","    Description:\n","    This function executes the core autoflattening operation\n","\n","    :param df: [type: pyspark.sql.dataframe.DataFrame] dataframe to be used for flattening\n","    :param json_column_name: [type: string] name of the column with json string\n","\n","    :return df: DataFrame containing flattened records\n","    '''\n","    # gets all fields of StructType or ArrayType in the nested_fields dictionary\n","    nested_fields = dict([\n","        (field.name, field.dataType)\n","        for field in df.schema.fields\n","        if isinstance(field.dataType, ArrayType) or isinstance(field.dataType, StructType)\n","    ])\n","\n","    # repeat until all nested_fields i.e. belonging to StructType or ArrayType are covered\n","    while nested_fields:\n","        # if there are any elements in the nested_fields dictionary\n","        if nested_fields:\n","            # get a column\n","            column_name = list(nested_fields.keys())[0]\n","            # if field belongs to a StructType, all child fields inside it are accessed\n","            # and are aliased with complete path to every child field\n","            if isinstance(nested_fields[column_name], StructType):\n","                unnested = [col(column_name + '.' + child).alias(column_name + '>' + child) for child in [ n.name for n in  nested_fields[column_name]]]\n","                df = df.select(\"*\", *unnested).drop(column_name)\n","            # else, if the field belongs to an ArrayType, an explode_outer is done\n","            elif isinstance(nested_fields[column_name], ArrayType):\n","                df = df.withColumn(column_name, explode_outer(column_name))\n","\n","        # Now that df is updated, gets all fields of StructType and ArrayType in a fresh nested_fields dictionary\n","        nested_fields = dict([\n","            (field.name, field.dataType)\n","            for field in df.schema.fields\n","            if isinstance(field.dataType, ArrayType) or isinstance(field.dataType, StructType)\n","        ])\n","\n","    # renaming all fields extracted with json> to retain complete path to the field\n","    for df_col_name in df.columns:\n","        df = df.withColumnRenamed(df_col_name, df_col_name.replace(\"transformedJSON\", json_column_name))\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"cf3417ac-7a4d-421b-8df0-c437a3adc5d6","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"","errorTraceType":null,"metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["def execute_autoflatten_json(input_df, json_column_name, spark_session):\n","    '''\n","    Description:\n","    This function executes the flattening of json records in the dataframe. It calls the `get_json_df` and `execute_autoflatten` functions to do this. \n","        :param input_df: [type: pyspark.sql.dataframe.DataFrame] input dataframe\n","        :param json_column_name: [type: string] name of the column with json string\n","        :param spark_session: SparkSession object\n","        :return unstd_df: contains flattened dataframe with unstandardized column name format\n","    '''\n","    json_df = get_json_df(input_df, json_column_name, spark_session)\n","    unstd_df = execute_autoflatten(json_df, json_column_name)\n","    return unstd_df"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"73943990-e70f-4132-8ba4-de3c25f307cf","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"","errorSummary":"","errorTraceType":null,"metadata":{},"type":"ipynbError"}},"output_type":"display_data"}],"source":["class Log4jWrapper(object):\n","    \"\"\"\n","    ------------------------------\n","    Description\n","    ------------------------------\n","    A class that creates a callable instance of the PySpark built in Log4j JVM logging object.\n","    Log4j is a Java-based logging utility part of the Apache Logging Services. \n","    \n","    ------------------------------\n","    Parameters\n","    ------------------------------\n","    spark:\n","        SparkSession object.\n","    \"\"\"\n","\n","    def __init__(self, spark):\n","        \"\"\"        \n","        ------------------------------\n","        Description\n","        ------------------------------\n","        Initialise the class with configuration varaibles and generate the custom log4j logger object.\n","        Will create the message prefix standard value to be used when logging errors, warnings or informational messages in this format.\n","            'Custom Message Logged from <applicationName: <ApplicationName> | applicationID: <ID> | notebookName: <NotebookName>>\n","        \n","        ------------------------------\n","        Parameters\n","        ------------------------------\n","        spark:\n","            The currently active SparkSession object        \n","            \n","        ------------------------------\n","        Return Value(s)\n","        ------------------------------\n","        None\n","        \n","        ------------------------------\n","        Example usage\n","        ------------------------------\n","        vLog4jWrapper = Log4jWrapper(spark)\n","\n","        vLog4jWrapper.info('Test info logging')\n","        vLog4jWrapper.error('Test error logging')\n","        vLog4jWrapper.warning('Test warning logging')\n","        \n","        \"\"\"\n","        # Get the currently active Spark Application details with which to prefix all messages. Store the details in variable(s) that other methods in this class can utilise\n","        self.sparkConfiguration = spark.sparkContext.getConf()\n","        self.applicationID = self.sparkConfiguration.get('spark.app.id')\n","        self.applicationName = self.sparkConfiguration.get('spark.app.name')\n","        \n","        # Get the notebook name where the class is being instantiated in. Store the details in variables that other methods in this class can utilise\n","        self.notebookName = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n","        \n","        #Create the custom message prefix to attach to all messages sent to the methods of this class. Store the details in variables that other methods in this class can utilise\n","        self.message_prefix = 'Custom Message Logged from <applicationName: ' + self.applicationName + ' | applicationID: ' + self.applicationID + ' | notebookName: ' + self.notebookName + '>: '\n","        \n","        #Get the reference to the Log4h object in this spark application\n","        log4j = spark._jvm.org.apache.log4j\n","        \n","        # Using the Log4j object, get an instance of the logger, passing in the custom message prefix defined above\n","        self.message_prefix = '<' + self.applicationName + ' ' + self.applicationID + ' ' + self.notebookName + '>'\n","        \n","        #Create the logger object instance used by all methods of this class\n","        self.logger = log4j.LogManager.getLogger(self.message_prefix)\n","\n","    def error(self, message):\n","        \"\"\"\n","        ------------------------------\n","        Description\n","        ------------------------------\n","        Log an error to the Log4j log using the logger defined int the init function.\n","        The error message should be clear as to what is most likely causing the problem. \n","        Provide as much context to the current state of the application as possible to help guide troubleshooting. \n","        \n","        ------------------------------\n","        Parameters\n","        ------------------------------\n","        message:\n","            The error message to log that describes the current state of the code, data, variables etc. \n","            \n","        ------------------------------\n","        Return Value(s)\n","        ------------------------------\n","        None        \n","        \"\"\"\n","        \n","        self.logger.error(message)\n","        \n","        return None\n","\n","    def warning(self, message):\n","        \"\"\"\n","        ------------------------------\n","        Description\n","        ------------------------------\n","        Log a Warning to the Log4j log using the logger defined int the init function.\n","        Warnings should state when best practices are not being followed which will not necesarily cause errors. \n","        \n","        ------------------------------\n","        Parameters\n","        ------------------------------\n","        message:\n","            The warning message to log that describes the current state of the code, data, variables etc. \n","            \n","        ------------------------------\n","        Return Value(s)\n","        ------------------------------\n","        None        \n","        \"\"\"        \n","\n","        self.logger.warn(message)\n","        \n","        return None\n","\n","    def info(self, message):\n","        \"\"\"\n","        ------------------------------\n","        Description\n","        ------------------------------\n","        Log an informational message to the Log4j log using the logger defined int the init function.\n","        Typically used for debugging/troubleshooting and general program state logging. \n","        Use to log variable values or the current location in the notebook to the Log4J log. \n","        \n","        ------------------------------\n","        Parameters\n","        ------------------------------\n","        message:\n","            The message to log that describes the current state of the code, data, variables etc. \n","            \n","        ------------------------------\n","        Return Value(s)\n","        ------------------------------\n","        None        \n","        \"\"\"            \n","\n","        self.logger.info(message)\n","        \n","        return None"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4,"widgetLayout":[]},"notebookName":"DataLakeHelperFunctions","notebookOrigID":984600405051123,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
