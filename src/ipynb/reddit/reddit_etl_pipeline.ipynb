{"cells":[{"cell_type":"markdown","source":["## Analysis of Reddit Comments\nI pulled the Dataset from [Reddit's Archive Site](https://archive.org/details/2015_reddit_comments_corpus), which contains a \"_Complete Public Reddit Comments Corpus_\".  \nReddit post with more [details of the dataset](https://www.reddit.com/r/datasets/comments/3bxlg7/i_have_every_publicly_available_reddit_comment/).  \n* 20 million API calls to generate over months\n* 1TB compressed JSON\n* 1.7 billion comments  \n\nI'll attempt to bring the dataset into my environment, perform an ETL on the dataset, and run LDA on it to determine the topics of them."],"metadata":{}},{"cell_type":"code","source":["# Install an init script to test boto3 import functionality to push data to an S3 bucket\ndbutils.fs.put(\"/databricks/init/mwc/install_boto3.sh\", \"\"\"\n#/bin/bash\npip install boto3\n\"\"\", True)"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["#### Parse Metadata\nThe following cells pull a metadata file to the dataset that we will use to analyze and load into our own S3 bucket for processing."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nimport pyspark.sql.functions as F\nimport xml.etree.ElementTree as ET\nimport requests\nimport shutil\n\nreddit_meta_url = 'https://ia801005.us.archive.org/19/items/2015_reddit_comments_corpus/2015_reddit_comments_corpus_files.xml'\nr = requests.get(reddit_meta_url, stream=True)\nif r.status_code == 200:\n    with open('/tmp/reddit_meta.xml', 'wb') as f:\n        r.raw.decode_content = True\n        shutil.copyfileobj(r.raw, f)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sh\nhead -n 10 /tmp/reddit_meta.xml "],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Load the Python XML parser to parse the filenames / URL from the metadata\nimport xml.etree.ElementTree as etree\nwith open('/tmp/reddit_meta.xml', 'r') as xml_file:\n    xml_tree = etree.parse(xml_file)\nroot = xml_tree.getroot()\n\n# Filter out the entries that contain the name tags that are not empty and get the file size tags\nfile_names = [elem.attrib['name'] for elem in root.iter() if 'name' in  elem.attrib]\nfile_sizes = [elem.text for elem in root.iter() if 'size' in elem.tag]"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Find the indices for the files that end with bzip2 compression\nindx = []\nfor f in file_names:\n  if \"bz2\" in f:\n    indx.append(file_names.index(f))\n\n# Parse out the names and sizes\nfnames = [file_names[i] for i in indx]\nfsizes = [file_sizes[i] for i in indx]\n\n# Zip the values up to create an RDD and convert it to a DataFrame\ndf = sc.parallelize(zip(fnames, fsizes)).toDF([\"fname\", \"fsize\"])"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndisplay(df.select(col(\"fsize\").alias(\"file size\"), col(\"fname\").alias(\"name\")))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\ndf_mb = df.withColumn('size_mb', df.fsize / 1000000)\ndisplay(df_mb.select(F.split(df_mb.fname, '/')[1].alias('year'), df_mb.size_mb.alias(\"Size in MB\")))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# 26 data files missing since they're > 3GB in size compressed\n# There is an 5GB object limit for S3, so I filtered out files over 3GB to test the multi-part upload api for S3\ndf_mb.filter(df_mb.size_mb >= 3000.0).count()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["display(df_mb.filter(df_mb.size_mb < 3000.0))"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Parallel Data Ingest\nThis is the beginning of the APIs used to pull the datasets into my S3 bucket."],"metadata":{}},{"cell_type":"code","source":["%run \"/Users/mwc@databricks.com/_helper.py\""],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["(ACCESS_KEY, SECRET_KEY) = get_creds()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["#### Boto File Upload APIs\nBoto implementation to support multi-part uploads for S3."],"metadata":{}},{"cell_type":"code","source":["import requests\n\ndef download_file(url):\n  local_filename = url.split('/')[-1]\n  local_fq_filename = \"/tmp/\" + local_filename\n  # NOTE the stream=True parameter\n  r = requests.get(url, stream=True)\n  with open(local_fq_filename, 'wb') as f:\n    for chunk in r.iter_content(chunk_size=2048): \n      if chunk: # filter out keep-alive new chunks\n        f.write(chunk)\n  return local_fq_filename\n\ndef upload_file_multipart(s3, bucketname, filepath, filename):\n    import os\n    import math\n\n    bkt = s3.get_bucket(bucketname)\n    k = bkt.new_key(filename)\n\n    mp = bkt.initiate_multipart_upload(filename)\n\n    source_size = os.stat(filepath).st_size\n    bytes_per_chunk = 5000*1024*1024\n    chunks_count = int(math.ceil(source_size / float(bytes_per_chunk)))\n\n    for i in range(chunks_count):\n            offset = i * bytes_per_chunk\n            remaining_bytes = source_size - offset\n            bytes = min([bytes_per_chunk, remaining_bytes])\n            part_num = i + 1\n\n            print \"uploading part \" + str(part_num) + \" of \" + str(chunks_count)\n\n            with open(filepath, 'r') as fp:\n                    fp.seek(offset)\n                    mp.upload_part_from_file(fp=fp, part_num=part_num, size=bytes)\n\n    if len(mp.get_all_parts()) == chunks_count:\n            mp.complete_upload()\n            print \"upload_file done\"\n    else:\n            mp.cancel_upload()\n            print \"upload_file failed\""],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["def get_files_to_s3_multipart(files):\n  # REF: http://codeinpython.blogspot.com/2015/08/s3-upload-large-files-to-amazon-using.html \n  # Import the necessary modules for the executors\n  import boto\n  import requests\n  import os\n  debug = True\n  bucket_name = get_bucket_name()\n  # Base URL to fetch the files\n  base_url = 'https://ia801005.us.archive.org/19/items/2015_reddit_comments_corpus/'\n  \n  # Register the S3 boto3 client for AWS\n  s3client = boto.connect_s3(\n    aws_access_key_id=ACCESS_KEY,\n    aws_secret_access_key=SECRET_KEY)\n  \n  bkt = s3client.get_bucket(bucket_name)\n\n  for r in files:\n    # Step 1: Get the initial file\n    # Build the full URL\n    file_url = base_url + r.fname\n    # Download file to local /tmp\n    download_file(file_url)\n    \n    # Step 2: Initiate multipart upload for large files \n    file_name = r.fname.split('/')[-1]\n    file_path = \"/tmp/\" + file_name\n    if debug:\n      print \"filepath: \" + file_path\n      print \"file_name: reddit/\" + file_name\n    upload_file_multipart(s3client, bucket_name, file_path, \"reddit/\" + file_name)\n    \n    # Delete local file and continue\n    os.remove(file_path)\n    "],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["#### Boto3 File Upload API\nBoto3 has a limitation with the 5GB limit for S3 uploads. I've created the multi-part api using the older boto library."],"metadata":{}},{"cell_type":"code","source":["# Print statements are seen in stderr on cluster executors\n# This process takes 2079.28s to complete the download and push to S3\ndef get_files_to_s3_boto3(files):\n  # Import the necessary modules for the executors\n  import boto3\n  import requests\n  import os\n  debug = True\n  bucketname = get_bucket_name()\n  # Register the S3 boto3 client for AWS\n  s3client = boto3.client('s3',\n    aws_access_key_id=ACCESS_KEY,\n    aws_secret_access_key=SECRET_KEY)\n  \n  # Base URL to fetch the files\n  base_url = 'https://ia801005.us.archive.org/19/items/2015_reddit_comments_corpus/'\n  for r in files:\n    # Build the full URL\n    file_url = base_url + r.fname\n    # Download file to local /tmp\n    download_file(file_url)\n    # Put file to S3 bucket\n    file_name = r.fname.split('/')[-1]\n    s3_obj_key = \"reddit/\" + file_name\n    s3client.put_object(Bucket=bucketname, Key=s3_obj_key, Body=open('/tmp/' + file_name, 'rb'))\n\n    # Delete local file and continue\n    os.remove(\"/tmp/\" + file_name)\n    "],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["#### Upload files < 5GB in Size"],"metadata":{}},{"cell_type":"code","source":["# Filter files < 3GB in total size to build out a DataFrame to perform the parallel ingestion\ndf_lt_3gb = df_mb.filter(df_mb.size_mb < 3000.0)\ndf_gt_3gb = df_mb.filter(df_mb.size_mb >= 3000.0)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# 2 Implementations to get around S3 issues. Multi-part upload used for larger objects.\ndf_lt_3gb.foreachPartition(get_files_to_s3)\ndf_gt_3gb.foreachPartition(get_files_to_s3_multipart)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["#### Calculate Total Size of Compressed Data"],"metadata":{}},{"cell_type":"code","source":["%scala\n// Calculate the size of the compressed dataset\nimport org.apache.spark.sql.functions._\n\nval df = dbutils.fs.ls(\"/mnt/mwc/reddit\").toDF()\nval c = df.agg(sum(\"size\")).collect()(0)\nval p = dbutils.fs.ls(\"/mnt/mwc/reddit_parquet\").toDF().agg(sum(\"size\")).collect()(0)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["## Read Reddit Parquet Data For Analysis\nWe've pulled the dataset into an S3 bucket to allow Spark to process it further.  \n* **Input**: json + bzip2 compression (50,687,364,160 = 50.5GB)\n* **Output**: Parquet + gzip (52,395,455,189 = 52.3GB)"],"metadata":{}},{"cell_type":"code","source":["# Takes 4836.81 seconds to do the ETL of \ndfr = sqlContext.read.json(\"/mnt/mwc/reddit/\")\ndfr.write.parquet(\"/mnt/mwc/reddit_parquet\")"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["dfp = sqlContext.read.parquet(\"/mnt/mwc/reddit_parquet\")\ndfp.registerTempTable(\"mwc_reddit\")\ndfp.printSchema()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["display(dfp)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["dfp.withColumn('year', F.year(F.from_unixtime(dfp.created_utc)))"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["### Partitioned Datasets\nLoading the complete dataset requires isn't always necessary, so I partitioned the dataset by the year of each post. I partitioned by year and we can easily create logical samples of the datasets using unions or views based on a subset of the years."],"metadata":{}},{"cell_type":"code","source":["# Create the new column called 'year' and we partition the new dataset. \ndf_p = dfp.withColumn('year', F.year(F.from_unixtime(dfp.created_utc)))\ndf_p.write.partitionBy('year').parquet('/mnt/mwc/reddit_year')"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### Working with Partitioned Datasets\nHere I've created temp tables to reference the partitioned datasets. We can read the partitions directly and register them as temp tables without overhead of using the Hive Metastore. \nHere I find the partitions, and loop through them creating `reddit_$year` tables."],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as F\nfrom pyspark.sql.types import *\n\n# To create the complete dataset, let's create temporary tables per year then find create a master union table\ndf = sc.parallelize(dbutils.fs.ls(\"/mnt/mwc/reddit_year\")).toDF()\n\n# Parse the year partition to get an array of years to register the tables by\nyears = df.select(F.regexp_extract('name', '(\\d+)', 1).alias('year')).collect()\nyear_partitions = [x.asDict().values()[0] for x in years if x.asDict().values()[0]]\nyear_partitions"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["# Loop over and register a table per year \nfor y in year_partitions:\n  df = sqlContext.read.parquet(\"/mnt/mwc/reddit_year/year=%s\" % y)\n  df.registerTempTable(\"reddit_%s\" % y)\n\n# Register the root directory for the complete dataset \ndf_complete = sqlContext.read.parquet(\"/mnt/mwc/reddit_year/\").registerTempTable(\"reddit_all\")\n  "],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":35}],"metadata":{"name":"Reddit ETL Pipeline","notebookId":245894},"nbformat":4,"nbformat_minor":0}