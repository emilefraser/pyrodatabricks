{"cells":[{"cell_type":"markdown","source":["# Mount an Azure Data Lake location to Databricks Cluster\n\nMounting an Data Lake location to the cluster is the preferred method of creating the connecting to the source data securely. \nNote, this is a one time operation. Once the mount point is created in the cluster, anyone that has access to the cluster will be able to use it. \nThis is ideal for security, since the Admin will create the mount point, and the data engineers/scientists can simply use it. Each individual doesn't have to provide authentication in each notebook each time. \n\nFor individual access to files in a Data Lake directly without a mount point, see [Access ADLS Gen2 directly](https://docs.databricks.com/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access.html#access-adls-gen2-directly)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d114fd27-a895-458f-b38d-fa764351b897"}}},{"cell_type":"markdown","source":["# Using Blob Endpoint and Storage Account Key"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28be1c46-7d11-45ba-8ec2-55c85f59925b"}}},{"cell_type":"markdown","source":["```\ndbutils.fs.mount(\nsource = \"wasbs://blob-container@bdpgen2datalake.blob.core.windows.net/blob-storage\",\nmount_point = \"/mnt/blob-storage\",\nextra_configs = {\"fs.azure.account.key.bdpgen2datalake.blob.core.windows.net\":dbutils.secrets.get(scope = \"databricks-secret-scope\",key = \"blob-container-key\")})\n```\n                                        \n                                        \ndbutils is automaically imported and available to the notebook, you dont need to explicitly import the library\n* fs.mount is the standard method used from this library to mount an external location to the cluster\n\n## Source\nNote, it appears that you cannot access the ABFSS Endpoint of the data lake using the storage account key. The code throws an error. You will need to use a Service Principle to do that. Thus, the below code is to access the blob storage api **wasbs** of the data lake using a storage account key. \n\n1. Pre-requisites\n\t1. Data Lake already defined\n\t2. Key Vault Backed Secret Scope already defined\n2. Create Mount in Azure Databricks in Python\n\t1. `dbutils.fs.mount(source = \"wasbs://blob-container@bdpgen2datalake.blob.core.windows.net/blob-storage\",mount_point = \"/mnt/blob-storage\",extra_configs = {\"fs.azure.account.key.bdpgen2datalake.blob.core.windows.net\":dbutils.secrets.get(scope = \"databricks-secret-scope\",key = \"blob-container-key\")})`\n\t2. Standard URL: `\"wasbs://blob-container@bdpgen2datalake.blob.core.windows.net/blob-storage\"`\n\t3. In general the below is true, **but it appears this code only works with the blob.core.windows.net and wasbs configs...**\n\t\t1. **dfs.core.windows.net** is used for Data Lakes\n\t\t2. **blob.core.windows.net** is used for Blob Storage\n\t\t3. **ABFS[S]** is used for Azure Data Lake Storage Gen2 which is based on normal Azure storage(during creating Azure storage account, enable Hierarchical namespace, then you create a Azure Data Lake Storage Gen2). An example is here.\n\t\t4. **WASB[S]** is used for the normal Azure storage. An example is here.\n3. For the conf-key (telling spark how you are authenticating...),\n\t1. use `fs.azure.account.key.<storage-account-name>.blob.core.windows.net` when you are using account key for auth\n\t2. use `fs.azure.sas.<container-name>.<storage-account-name>.blob.core.windows.net` when you are using an SAS TOken for auth"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c1d4202-1d60-485b-a8dc-a9adb77b91e0"}}},{"cell_type":"code","source":["#mount the location\ndbutils.fs.mount(\n  source = \"wasbs://rawdata@dianrandddatalake.blob.core.windows.net/MockCSVFiles\", #URL to the external location you want to mount in the data lake\n  mount_point = \"/mnt/datalake_rawdata_MockCSVFiles\", #location inside the databricks file system this mount point will be accessed from in future. mnt/ is standard protocol. \n  extra_configs = {\"fs.azure.account.key.dianrandddatalake.blob.core.windows.net\" : dbutils.secrets.get(scope = \"DianGRAndDKeyVault\", key = \"dianrandddatalake-accountkey\")}\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"664c9696-bfbc-4c19-a2f7-55654be1d9a3"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[9]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[9]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["#unmount the location\ndbutils.fs.unmount(\"/mnt/datalake_rawdata_MockCSVFiles\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b0df1cc-1d52-4fe8-90f1-357d4429a527"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/mnt/datalake_rawdata_MockCSVFiles has been unmounted.\nOut[10]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/datalake_rawdata_MockCSVFiles has been unmounted.\nOut[10]: True</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Using ADLS Endpoint and Service Principle (Recommended)\n\n* It appears the only way to mount using the ADLS2 api is to use a service principle. \n* You can use the below code as a standard in all notebooks as no sensitive information is stored here. \n* All sensitive information is retrieved from key vault. \n* This can actually be converted into a parameterised notebook called at the start of other notebooks. \n* Just need to send the new mount point directory in as the mount target..."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b93d2fc9-1cd3-4045-a91a-5c80351600c7"}}},{"cell_type":"markdown","source":["## Mount a container as a whole - preferred"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"45438b29-1cdd-4a8a-9752-8917aff1a1dd"}}},{"cell_type":"code","source":["# Python code to mount and access Azure Data Lake Storage Gen2 Account from Azure Databricks with Service Principal and OAuth\n\n# KeyVault Secret Scope Name\nVarSecretScopeName = \"DianGRAndDKeyVault\" ##this would be a fixed name we would have a standard for. Ideally there is only one secret scope for autoamted notebooks\n \n# Define the variables used for creating connection strings - Data Lake Related\nvarAdlsAccountName = dbutils.secrets.get(scope=VarSecretScopeName,key=\"dianrandddatalake-storageaccountname\") # e.g. \"dianrandddatalake\" --the storage account name itself\nvarAdlsContainerName = \"rawdata\" #Would be parameterised based on what the notebook is doing, for now just hardcoding\nvarMountPoint = \"/mnt/datalake_\" + varAdlsContainerName #Would be parameterised based on what the notebook is doing - datalake_<adlsContainerName>\n\n# Define the variables that have the names of the secrets in key vault that store the sensitive information we need for the conenction via Service Principle Auth\nVarSecretClientID = \"RandD-ServicePrinciple-ApplicationID\" #Name of the generic key vault secret contianing the Service Principle Name.\nVarSecretClientSecret = \"RandD-ServicePrinciple-Password\" #Name of the generic key vault secret contianing the Service Principle Password. \nVarSecretTenantID = \"RandD-ServicePrinciple-TenantID\" #Bame of the generic key vault secret contianing the Tenant ID.\n\n# Get the actual secrets from key vault for the service principle\nvarApplicationId = dbutils.secrets.get(scope=VarSecretScopeName, key=VarSecretClientID) # Application (Client) ID\nvarAuthenticationKey = dbutils.secrets.get(scope=VarSecretScopeName, key=VarSecretClientSecret) # Application (Client) Secret Key\nvarTenantId = dbutils.secrets.get(scope=VarSecretScopeName, key=VarSecretTenantID) # Directory (Tenant) ID\n\n# Using the secrets above, generate the URL to the storage account and the authentication endpoint for OAuth\nvarEndpoint = \"https://login.microsoftonline.com/\" + varTenantId + \"/oauth2/token\" #Fixed URL for the endpoint\nvarSource = \"abfss://\" + varAdlsContainerName + \"@\" + varAdlsAccountName + \".dfs.core.windows.net/\"\n \n# Connecting using Service Principal secrets and OAuth\nvarConfigs = {\"fs.azure.account.auth.type\": \"OAuth\", #standard\n           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\", #standard\n           \"fs.azure.account.oauth2.client.id\": varApplicationId,\n           \"fs.azure.account.oauth2.client.secret\": varAuthenticationKey,\n           \"fs.azure.account.oauth2.client.endpoint\": varEndpoint}\n \n# Mount ADLS Storage to DBFS only if the directory is not already mounted\n# Mount is generated as a list of all mount points available already via dbutils.fs.mounts()\n# Then it checks the list for the new mount point we are trying to generate.\nif not any(mount.mountPoint == varMountPoint for mount in dbutils.fs.mounts()): \n  dbutils.fs.mount(\n    source = varSource,\n    mount_point = varMountPoint,\n    extra_configs = varConfigs)\n\n# print the mount point used for troubleshooting\nprint(\"Mount Point: \" + varMountPoint)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f127bfa-fcec-4990-9813-79f656679108"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Mount Point: /mnt/datalake_rawdata\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Mount Point: /mnt/datalake_rawdata\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Mount a specific directory inside a container"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce82f375-e431-4b5e-ac42-126ecca46588"}}},{"cell_type":"code","source":["# Python code to mount and access Azure Data Lake Storage Gen2 Account from Azure Databricks with Service Principal and OAuth\n\n# KeyVault Secret Scope Name\nVarSecretScopeName = \"DianGRAndDKeyVault\" ##this would be a fixed name we would have a standard for. Ideally there is only one secret scope for autoamted notebooks\n \n# Define the variables used for creating connection strings - Data Lake Related\nvarAdlsAccountName = dbutils.secrets.get(scope=VarSecretScopeName,key=\"dianrandddatalake-storageaccountname\") # e.g. \"dianrandddatalake\" --the storage account name itself\nvarAdlsContainerName = \"rawdata\" #Would be parameterised based on what the notebook is doing, for now just hardcoding\nvarAdlsFolderName = \"MockCSVFiles\" #Would be parameterised based on what the notebook is doing, for now just hardcoding\nvarMountPoint = \"/mnt/datalake_rawdata_MockCSVFiles\" #Would be parameterised based on what the notebook is doing - datalake_<adlsContainerName>_<adlsFolderName>\n\n# Define the variables that have the names of the secrets in key vault that store the sensitive information we need for the conenction via Service Principle Auth\nVarSecretClientID = \"RandD-ServicePrinciple-ApplicationID\" #Name of the generic key vault secret contianing the Service Principle Name.\nVarSecretClientSecret = \"RandD-ServicePrinciple-Password\" #Name of the generic key vault secret contianing the Service Principle Password. \nVarSecretTenantID = \"RandD-ServicePrinciple-TenantID\" #Bame of the generic key vault secret contianing the Tenant ID.\n\n# Get the actual secrets from key vault for the service principle\nvarApplicationId = dbutils.secrets.get(scope=VarSecretScopeName, key=VarSecretClientID) # Application (Client) ID\nvarAuthenticationKey = dbutils.secrets.get(scope=VarSecretScopeName, key=VarSecretClientSecret) # Application (Client) Secret Key\nvarTenantId = dbutils.secrets.get(scope=VarSecretScopeName, key=VarSecretTenantID) # Directory (Tenant) ID\n\n# Using the secrets above, generate the URL to the storage account and the authentication endpoint for OAuth\nvarEndpoint = \"https://login.microsoftonline.com/\" + varTenantId + \"/oauth2/token\" #Fixed URL for the endpoint\nvarSource = \"abfss://\" + varAdlsContainerName + \"@\" + varAdlsAccountName + \".dfs.core.windows.net/\" + varAdlsFolderName\n \n# Connecting using Service Principal secrets and OAuth\nvarConfigs = {\"fs.azure.account.auth.type\": \"OAuth\", #standard\n           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\", #standard\n           \"fs.azure.account.oauth2.client.id\": varApplicationId,\n           \"fs.azure.account.oauth2.client.secret\": varAuthenticationKey,\n           \"fs.azure.account.oauth2.client.endpoint\": varEndpoint}\n \n# Mount ADLS Storage to DBFS only if the directory is not already mounted\n# Mount is generated as a list of all mount points available already via dbutils.fs.mounts()\n# Then it checks the list for the new mount point we are trying to generate.\nif not any(mount.mountPoint == varMountPoint for mount in dbutils.fs.mounts()): \n  dbutils.fs.mount(\n    source = varSource,\n    mount_point = varMountPoint,\n    extra_configs = varConfigs)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb8fccad-5a40-4859-b942-6c011d988e46"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9f68b2f-1cde-4877-b748-dbf9c94ce667"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Mount an Azure Data Lake location to Databricks Cluster","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":3068325291162488}},"nbformat":4,"nbformat_minor":0}
