{"cells":[{"cell_type":"markdown","source":["# 03b - Parallel Switch-In Load Into Partitioned Table - Sigle Partition Load\n\nThis notebook will bulk load data into exactly one Azure SQL partition. It accepts a Partition Key as a parameter, and that value will be used to load all data that belongs to that partition. In this sample column used to partition data is the `L_PARTITION_KEY` column, which is an integer, so the provided partition key *must be* an integer too.\n\nData is not loaded directly into the selected partition, but a staging table is created, loaded and then switched into the target table, becoming the desired partition.\n\nMore info on this switch-in technique can be found in the related notebook: `03a-parallel-switch-in-load-into-partitioned-table-many`\n\nThe sample is using the new sql-spark-connector (https://github.com/microsoft/sql-spark-connector). Make sure you have installed it before running this notebook.\n\n## Notes on terminology\n\nThe term \"row-store\" is used to identify and index that is not using the [column-store layout](https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview) to store its data.\n\n## Sample\n\nThis notebook is used to load exactly on partition of a partitioned table by loading a staging table and then switching it in into the target table. The process is the following:\n\n- Create a staging table\n- Load staging table\n- Create indexes\n- Create check constraints\n- Execute switch-in operation\n\nMore details on this pattern can be found in [this post](https://www.cathrinewilhelmsen.net/2015/04/19/table-partitioning-in-sql-server-partition-switching/) written by the Data Platform MVP Cathrine Wilhelmsen. \n\n## Supported Azure Databricks Versions\n\nDatabricks supported versions: Spark 2.4.5 and Scala 2.11"],"metadata":{}},{"cell_type":"markdown","source":["## Setup"],"metadata":{}},{"cell_type":"markdown","source":["Define notebook parameter:"],"metadata":{}},{"cell_type":"code","source":["dbutils.widgets.text(\"partitionKey\", \"0\", \"Partition Key\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["Define variables used thoughout the script. Azure Key Value has been used to securely store sensitive data. More info here: [Create an Azure Key Vault-backed secret scope](https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope)"],"metadata":{}},{"cell_type":"code","source":["val partitionKey = dbutils.widgets.get(\"partitionKey\").toInt\nval prevPartitionKey = partitionKey\n\nval scope = \"key-vault-secrets\"\n\nval storageAccount = \"dmstore2\";\nval storageKey = dbutils.secrets.get(scope, \"dmstore2-2\");\n\nval server = dbutils.secrets.get(scope, \"srv001\").concat(\".database.windows.net\");\nval database = \"ApacheSpark\";\nval user = dbutils.secrets.get(scope, \"dbuser001\");\nval password = dbutils.secrets.get(scope, \"dbpwd001\");\nval table = \"dbo.LINEITEM_LOADTEST\"\n\nval url = s\"jdbc:sqlserver://$server;databaseName=$database;\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">partitionKey: Int = 199810\nprevPartitionKey: Int = 199810\nscope: String = key-vault-secrets\nstorageAccount: String = dmstore2\nstorageKey: String = [REDACTED]\nserver: String = [REDACTED].database.windows.net\ndatabase: String = ApacheSpark\nuser: String = [REDACTED]\npassword: String = [REDACTED]\ntable: String = dbo.LINEITEM_LOADTEST\nurl: String = jdbc:sqlserver://[REDACTED].database.windows.net;databaseName=ApacheSpark;\n</div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["Configure Spark to access Azure Blob Store"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(s\"fs.azure.account.key.$storageAccount.blob.core.windows.net\", storageKey);"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["Load the Parquet file generated in `00-create-parquet-file` notebook that contains LINEITEM data partitioned by Year and Month. Make sure only the specified partion is loaded"],"metadata":{}},{"cell_type":"code","source":["val li = spark\n  .read\n  .parquet(s\"wasbs://tpch@$storageAccount.blob.core.windows.net/10GB/parquet/lineitem\")\n  .filter($\"L_PARTITION_KEY\" === partitionKey)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">li: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [L_ORDERKEY: int, L_PARTKEY: int ... 15 more fields]\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["Create the T-SQL script need to extract information on the partition that will be loaded into Azure SQL"],"metadata":{}},{"cell_type":"code","source":["val sqlPartitionValueInfo = \ns\"\"\"\nSELECT\n\t*\nFROM\n(\n\tSELECT\n\t\tprv.[boundary_id] AS partitionId,\n\t\tCAST(prv.[value] AS INT) AS [value],\n\t\tCAST(LAG(prv.[value]) OVER (ORDER BY prv.[boundary_id]) AS INT) AS [prevValue],\n\t\tCAST(LEAD(prv.[value]) OVER (ORDER BY prv.[boundary_id]) AS INT) AS [nextValue]\n\tFROM\n\t\tsys.[indexes] i\n\tINNER JOIN\n\t\tsys.[data_spaces] dp ON i.[data_space_id] = dp.[data_space_id]\n\tINNER JOIN\n\t\tsys.[partition_schemes] ps ON dp.[data_space_id] = ps.[data_space_id]\n\tINNER JOIN\n\t\tsys.[partition_range_values] prv ON [prv].[function_id] = [ps].[function_id]\n\tWHERE\n\t\ti.[object_id] = OBJECT_ID('${table}')\n\tAND\n\t\ti.[index_id] IN (0,1)\n) AS [pi]\nWHERE\n\t[value] = ${partitionKey}\n\"\"\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">sqlPartitionValueInfo: String =\n&quot;\nSELECT\n\t*\nFROM\n(\n\tSELECT\n\t\tprv.[boundary_id] AS partitionId,\n\t\tCAST(prv.[value] AS INT) AS [value],\n\t\tCAST(LAG(prv.[value]) OVER (ORDER BY prv.[boundary_id]) AS INT) AS [prevValue],\n\t\tCAST(LEAD(prv.[value]) OVER (ORDER BY prv.[boundary_id]) AS INT) AS [nextValue]\n\tFROM\n\t\tsys.[indexes] i\n\tINNER JOIN\n\t\tsys.[data_spaces] dp ON i.[data_space_id] = dp.[data_space_id]\n\tINNER JOIN\n\t\tsys.[partition_schemes] ps ON dp.[data_space_id] = ps.[data_space_id]\n\tINNER JOIN\n\t\tsys.[partition_range_values] prv ON [prv].[function_id] = [ps].[function_id]\n\tWHERE\n\t\ti.[object_id] = OBJECT_ID('dbo.LINEITEM_LOADTEST')\n\tAND\n\t\ti.[index_id] IN (0,1)\n) AS [pi]\nWHERE\n\t[value] = 199810\n&quot;\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Setup JDBC connection, needed to execute ad-hoc T-SQL statement on Azure SQL"],"metadata":{}},{"cell_type":"code","source":["val connectionProperties = new java.util.Properties()\nconnectionProperties.put(\"user\", user)\nconnectionProperties.put(\"password\", password)\nconnectionProperties.setProperty(\"Driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\nval conn = java.sql.DriverManager.getConnection(url, connectionProperties)\nval st = conn.createStatement()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">connectionProperties: java.util.Properties = {user=[REDACTED], password=[REDACTED], Driver=com.microsoft.sqlserver.jdbc.SQLServerDriver}\nconn: java.sql.Connection = ConnectionID:18 ClientConnectionId: cba0a4b8-ec8c-419a-9158-f970f6cf3bb4\nst: java.sql.Statement = SQLServerStatement:35\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["Load Azure SQL partition metadata"],"metadata":{}},{"cell_type":"code","source":["case class PartitionInfo(partitionId: Int, value: Int, prevValue: Option[Int], nextValue: Option[Int]);\nval piDF = spark.read.jdbc(url, s\"($sqlPartitionValueInfo) AS t\", connectionProperties)\ndisplay(piDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>partitionId</th><th>value</th><th>prevValue</th><th>nextValue</th></tr></thead><tbody><tr><td>82</td><td>199810</td><td>199809</td><td>null</td></tr></tbody></table></div>"]}}],"execution_count":16},{"cell_type":"code","source":["val pi = piDF.as[PartitionInfo].collect()(0)\nprint(pi)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">PartitionInfo(82,199810,Some(199809),None)pi: PartitionInfo = PartitionInfo(82,199810,Some(199809),None)\n</div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["Create on Azure SQL a staging table where data will be bulk loaded"],"metadata":{}},{"cell_type":"code","source":["st.execute(s\"DROP TABLE IF EXISTS ${table}_STG_${partitionKey}\")\nst.execute(s\"SELECT TOP (0) * INTO ${table}_STG_${partitionKey} FROM ${table}\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res24: Boolean = false\n</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["Load the staging table"],"metadata":{}},{"cell_type":"code","source":["li.write \n  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n  .mode(\"overwrite\")   \n  .option(\"truncate\", \"true\") \n  .option(\"url\", url) \n  .option(\"dbtable\", s\"${table}_STG_${partitionKey}\") \n  .option(\"user\", user) \n  .option(\"password\", password) \n  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n  .option(\"tableLock\", \"false\") \n  .option(\"batchsize\", \"100000\")   \n  .option(\"schemaCheckEnabled\", \"false\")\n  .save()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"markdown","source":["Create the same indexes that the target table has, in order to allow switch-in"],"metadata":{}},{"cell_type":"code","source":["st.execute(s\"CREATE CLUSTERED INDEX IXC ON ${table}_STG_${partitionKey} ([L_COMMITDATE], [L_PARTITION_KEY])\")\nst.execute(s\"CREATE UNIQUE NONCLUSTERED INDEX IX1 ON ${table}_STG_${partitionKey} ([L_ORDERKEY], [L_LINENUMBER], [L_PARTITION_KEY])\")\nst.execute(s\"CREATE NONCLUSTERED INDEX IX2 ON ${table}_STG_${partitionKey} ([L_PARTKEY], [L_PARTITION_KEY])\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res26: Boolean = false\n</div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["Add a check constraint on the table to allow switch-in"],"metadata":{}},{"cell_type":"code","source":["if (pi.prevValue == None) {\n  st.execute(s\"ALTER TABLE ${table}_STG_${partitionKey} ADD CONSTRAINT ck_partition_${partitionKey} CHECK (L_PARTITION_KEY <= ${pi.value})\")\n} else {\n  st.execute(s\"ALTER TABLE ${table}_STG_${partitionKey} ADD CONSTRAINT ck_partition_${partitionKey} CHECK (L_PARTITION_KEY > ${pi.prevValue.get} AND L_PARTITION_KEY <= ${pi.value})\")\n}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res27: Boolean = false\n</div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["Delete data in existing partition of target table, execute the switch-in and drop the staging table"],"metadata":{}},{"cell_type":"code","source":["st.execute(s\"TRUNCATE TABLE ${table} WITH (PARTITIONS (${pi.partitionId}))\")\nst.execute(s\"ALTER TABLE ${table}_STG_${partitionKey} SWITCH TO ${table} PARTITION ${pi.partitionId}\")\nst.execute(s\"DROP TABLE ${table}_STG_${partitionKey}\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res122: Boolean = false\n</div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["Done!"],"metadata":{}},{"cell_type":"code","source":["dbutils.notebook.exit(partitionKey.toString)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/plain":["199810"]}}],"execution_count":29}],"metadata":{"name":"03b-parallel-switch-in-load-into-partitioned-table-single","notebookId":964636935775860},"nbformat":4,"nbformat_minor":0}