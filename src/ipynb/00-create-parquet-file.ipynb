{"cells":[{"cell_type":"markdown","source":["# 00 - Create Parquet file used in subsequent samples\n\nThis notebook will create the parquet file used in subsequent samples"],"metadata":{}},{"cell_type":"markdown","source":["Define variables used thoughout the script. Azure Key Value has been used to securely store sensitive data. More info here: [Create an Azure Key Vault-backed secret scope](https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope)"],"metadata":{}},{"cell_type":"code","source":["val scope = \"key-vault-secrets\"\n\nval storageAccount = \"dmstore2\";\nval storageKey = dbutils.secrets.get(scope, \"dmstore2-2\");\n\nval parquetLocation = s\"wasbs://tpch@$storageAccount.blob.core.windows.net/10GB/parquet/lineitem\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">scope: String = key-vault-secrets\nstorageAccount: String = dmstore2\nstorageKey: String = [REDACTED]\nparquetLocation: String = wasbs://tpch@dmstore2.blob.core.windows.net/10GB/parquet/lineitem\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["Configure Spark to access Azure Blob Store"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(s\"fs.azure.account.key.$storageAccount.blob.core.windows.net\", storageKey);"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["Load data from generated 10GB TPC-H LINEITEM file. Tools to generate TPC-H data can be found here:\nhttp://www.tpc.org/tpc_documents_current_versions/current_specifications5.asp"],"metadata":{}},{"cell_type":"code","source":["import org.apache.spark.sql.types._\n\nval li1 = spark\n  .read\n  .format(\"csv\")\n  .option(\"sep\", \"|\")\n  .schema(\"\"\"\n    L_ORDERKEY INTEGER,\n    L_PARTKEY INTEGER,\n    L_SUPPKEY INTEGER,\n    L_LINENUMBER INTEGER,\n    L_QUANTITY DECIMAL(15,2),\n    L_EXTENDEDPRICE DECIMAL(15,2),\n    L_DISCOUNT DECIMAL(15,2),\n    L_TAX DECIMAL(15,2),\n    L_RETURNFLAG CHAR(1),\n    L_LINESTATUS CHAR(1),\n    L_SHIPDATE DATE,\n    L_COMMITDATE DATE,\n    L_RECEIPTDATE DATE,\n    L_SHIPINSTRUCT CHAR(25),\n    L_SHIPMODE CHAR(10),\n    L_COMMENT VARCHAR(44)\n   \"\"\")\n  .load(s\"wasbs://tpch@$storageAccount.blob.core.windows.net/10GB/lineitem.tbl\")\n;"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import org.apache.spark.sql.types._\nli1: org.apache.spark.sql.DataFrame = [L_ORDERKEY: int, L_PARTKEY: int ... 14 more fields]\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["Create a temporary view to make it easier to manipulate schema and data"],"metadata":{}},{"cell_type":"code","source":["li1.createOrReplaceTempView(\"LINEITEM\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["Add a new column that will be used for partitioning"],"metadata":{}},{"cell_type":"code","source":["var li2 = spark.sql(\"SELECT *, YEAR(L_COMMITDATE) * 100 + MONTH(L_COMMITDATE) AS L_PARTITION_KEY FROM LINEITEM\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">li2: org.apache.spark.sql.DataFrame = [L_ORDERKEY: int, L_PARTKEY: int ... 15 more fields]\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["Repartition data using the newly created column"],"metadata":{}},{"cell_type":"code","source":["val li3 = li2.repartition($\"L_PARTITION_KEY\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">li3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [L_ORDERKEY: int, L_PARTKEY: int ... 15 more fields]\n</div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["Save dataframe into parquet format, making sure parquet will be saved using the same partitioning logic used for the dataframe"],"metadata":{}},{"cell_type":"code","source":["li3.write.mode(\"overwrite\").partitionBy(\"L_PARTITION_KEY\").parquet(parquetLocation)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["As a test read back the parquet files"],"metadata":{}},{"cell_type":"code","source":["val li4 = spark.read.parquet(parquetLocation)\nli4.rdd.getNumPartitions"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">li4: org.apache.spark.sql.DataFrame = [L_ORDERKEY: int, L_PARTKEY: int ... 15 more fields]\nres6: Int = 20\n</div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["Peek at first 10 partitions"],"metadata":{}},{"cell_type":"code","source":["display(li4.groupBy($\"L_PARTITION_KEY\").count().orderBy($\"L_PARTITION_KEY\").limit(10))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>L_PARTITION_KEY</th><th>count</th></tr></thead><tbody><tr><td>199201</td><td>412</td></tr><tr><td>199202</td><td>190252</td></tr><tr><td>199203</td><td>582150</td></tr><tr><td>199204</td><td>748645</td></tr><tr><td>199205</td><td>770266</td></tr><tr><td>199206</td><td>746006</td></tr><tr><td>199207</td><td>772006</td></tr><tr><td>199208</td><td>770213</td></tr><tr><td>199209</td><td>748997</td></tr><tr><td>199210</td><td>771256</td></tr></tbody></table></div>"]}}],"execution_count":19}],"metadata":{"name":"00-create-parquet-file","notebookId":1331848450253195},"nbformat":4,"nbformat_minor":0}