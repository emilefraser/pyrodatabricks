{"cells":[{"cell_type":"markdown","source":["# 02 - Load data into an Azure SQL partitioned table\n\nAzure SQL supports [table and index partitioning](https://docs.microsoft.com/en-us/sql/relational-databases/partitions/partitioned-tables-and-indexes). If a table is partitioned, data can be loaded in parallel without the need to put a lock on the entire table. In order to allow parallel partitions to be loaded, the source RDD/DataFrame/Dataset and the target Azure SQL table *MUST* have compatible partitions, which means that one RDD partition ends up exactly in one or more than one Azure SQL partitions, and those are not used by other RDD partitions.\n\nWhen table is partitioned, data *can* be bulk loaded in parallel also if there are indexes on the table. Especially on very large databases _this is the recommended approach_. The bulk load process will be a bit slower, but you'll not need to create indexes after having loaded the data. Creation of indexes on huge, already loaded, tables is a very expensive operation that you would like to avoid if possibile.\n\nThe sample is using the new sql-spark-connector (https://github.com/microsoft/sql-spark-connector). Make sure to have it installed in the cluster before running the notebook.\n\n## Dataframe and Azure SQL partitions\n\nBoth Azure SQL and Azure Databricks (more specifically, Spark, and even more specifically a Spark Dataframe) are able to use take advantage of partitioning to more easily deal with large amounts of data. Partitions allow to work on subset of data, and usually you can do such work in parallel, spreading the workload on several CPU and/or nodes.\n\n## Notes on terminology\n\nThe term \"row-store\" is used to identify and index that is not using the [column-store layout](https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview) to store its data.\n\n## Samples\n\nIn this notebook there are two samples\n\n- Load data into a partitioned table with row-store indexes\n- Load data into a partitioned table with columns-store indexes\n\n## Supported Azure Databricks Versions\n\nDatabricks supported versions: Spark 2.4.5 and Scala 2.11"],"metadata":{}},{"cell_type":"markdown","source":["## Setup"],"metadata":{}},{"cell_type":"markdown","source":["Define variables used thoughout the script. Azure Key Value has been used to securely store sensitive data. More info here: [Create an Azure Key Vault-backed secret scope](https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes#--create-an-azure-key-vault-backed-secret-scope)"],"metadata":{}},{"cell_type":"code","source":["val scope = \"key-vault-secrets\"\n\nval storageAccount = \"dmstore2\";\nval storageKey = dbutils.secrets.get(scope, \"dmstore2-2\");\n\nval server = dbutils.secrets.get(scope, \"srv001\").concat(\".database.windows.net\");\nval database = \"ApacheSpark\";\nval user = dbutils.secrets.get(scope, \"dbuser001\");\nval password = dbutils.secrets.get(scope, \"dbpwd001\");\n\nval url = s\"jdbc:sqlserver://$server;databaseName=$database;\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">scope: String = key-vault-secrets\nstorageAccount: String = dmstore2\nstorageKey: String = [REDACTED]\nserver: String = [REDACTED].database.windows.net\ndatabase: String = ApacheSpark\nuser: String = [REDACTED]\npassword: String = [REDACTED]\nurl: String = jdbc:sqlserver://[REDACTED].database.windows.net;databaseName=ApacheSpark;\n</div>"]}}],"execution_count":4},{"cell_type":"markdown","source":["Configure Spark to access Azure Blob Store"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(s\"fs.azure.account.key.$storageAccount.blob.core.windows.net\", storageKey);"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"markdown","source":["Load the Parquet file generated in `00-create-parquet-file` notebook that contains LINEITEM data partitioned by Year and Month"],"metadata":{}},{"cell_type":"code","source":["val li = spark.read.parquet(s\"wasbs://tpch@$storageAccount.blob.core.windows.net/10GB/parquet/lineitem\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">li: org.apache.spark.sql.DataFrame = [L_ORDERKEY: int, L_PARTKEY: int ... 15 more fields]\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["Loaded data is split in 20 dataframe partitions"],"metadata":{}},{"cell_type":"code","source":["li.rdd.getNumPartitions"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res2: Int = 20\n</div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["Show schema of loaded data"],"metadata":{}},{"cell_type":"code","source":["li.printSchema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- L_ORDERKEY: integer (nullable = true)\n-- L_PARTKEY: integer (nullable = true)\n-- L_SUPPKEY: integer (nullable = true)\n-- L_LINENUMBER: integer (nullable = true)\n-- L_QUANTITY: decimal(15,2) (nullable = true)\n-- L_EXTENDEDPRICE: decimal(15,2) (nullable = true)\n-- L_DISCOUNT: decimal(15,2) (nullable = true)\n-- L_TAX: decimal(15,2) (nullable = true)\n-- L_RETURNFLAG: string (nullable = true)\n-- L_LINESTATUS: string (nullable = true)\n-- L_SHIPDATE: date (nullable = true)\n-- L_COMMITDATE: date (nullable = true)\n-- L_RECEIPTDATE: date (nullable = true)\n-- L_SHIPINSTRUCT: string (nullable = true)\n-- L_SHIPMODE: string (nullable = true)\n-- L_COMMENT: string (nullable = true)\n-- L_PARTITION_KEY: integer (nullable = true)\n\n</div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["Make sure you create on your Azure SQL the following `LINEITEM` table, partitioned by `L_PARTITION_KEY`:\n\n```sql\ncreate partition function pf_LINEITEM(int)\nas range left for values \n(\n\t199201,199202,199203,199204,199205,199206,199207,199208,199209,199210,199211,199212,\n\t199301,199302,199303,199304,199305,199306,199307,199308,199309,199310,199311,199312,\n\t199401,199402,199403,199404,199405,199406,199407,199408,199409,199410,199411,199412,\n\t199501,199502,199503,199504,199505,199506,199507,199508,199509,199510,199511,199512,\n\t199601,199602,199603,199604,199605,199606,199607,199608,199609,199610,199611,199612,\n\t199701,199702,199703,199704,199705,199706,199707,199708,199709,199710,199711,199712,\n\t199801,199802,199803,199804,199805,199806,199807,199808,199809,199810\n);\n\ncreate partition scheme ps_LINEITEM\nas partition pf_LINEITEM\nall to ([Primary])\n;\n\ndrop table if exists [dbo].[LINEITEM_LOADTEST];\ncreate table [dbo].[LINEITEM_LOADTEST]\n(\n\t[L_ORDERKEY] [int] not null,\n\t[L_PARTKEY] [int] not null,\n\t[L_SUPPKEY] [int] not null,\n\t[L_LINENUMBER] [int] not null,\n\t[L_QUANTITY] [decimal](15, 2) not null,\n\t[L_EXTENDEDPRICE] [decimal](15, 2) not null,\n\t[L_DISCOUNT] [decimal](15, 2) not null,\n\t[L_TAX] [decimal](15, 2) not null,\n\t[L_RETURNFLAG] [char](1) not null,\n\t[L_LINESTATUS] [char](1) not null,\n\t[L_SHIPDATE] [date] not null,\n\t[L_COMMITDATE] [date] not null,\n\t[L_RECEIPTDATE] [date] not null,\n\t[L_SHIPINSTRUCT] [char](25) not null,\n\t[L_SHIPMODE] [char](10) not null,\n\t[L_COMMENT] [varchar](44) not null,\n\t[L_PARTITION_KEY] [int] not null\n) on ps_LINEITEM([L_PARTITION_KEY])\n```"],"metadata":{}},{"cell_type":"markdown","source":["You can check that Azure SQL table is partitioned by running the following T-SQL command:\n\n```sql\nSELECT\n    schema_name(t.schema_id) as [schema_name],\n    t.[name] as table_name,\n    i.[name] as index_name,\n    ps.[partition_id],\n    ps.partition_number,\n    p.data_compression_desc,\n    i.[type_desc],    \n    ps.row_count,\n    (ps.used_page_count * 8.) / 1024. / 1024. as size_in_gb\nfrom\n    sys.dm_db_partition_stats as ps \ninner join  \n    sys.partitions as p on ps.partition_id = p.partition_id\ninner join\n    sys.tables as t on t.object_id = ps.object_id\ninner join\n    sys.indexes as i on ps.object_id = i.object_id and ps.index_id = i.index_id\nwhere\n    t.[name] = 'LINEITEM_LOADTEST' and t.[schema_id] = schema_id('dbo')\norder by\n    [schema_name], table_name, index_name, partition_number\n```"],"metadata":{}},{"cell_type":"markdown","source":["## Load data into a partitioned table with row-store indexes"],"metadata":{}},{"cell_type":"markdown","source":["On the target table create the Clustered Index and a couple of Non-Clustered Index. In order to allow parallel partitioned load, also indexes must use the same partitioning function used by the table\n\n```sql\ncreate clustered index IXC on dbo.[LINEITEM_LOADTEST] ([L_COMMITDATE]) \non ps_LINEITEM([L_PARTITION_KEY]);\n\ncreate unique nonclustered index IX1 on dbo.[LINEITEM_LOADTEST] ([L_ORDERKEY], [L_LINENUMBER], [L_PARTITION_KEY]) \non ps_LINEITEM([L_PARTITION_KEY]);\n\ncreate nonclustered index IX2 on dbo.[LINEITEM_LOADTEST] ([L_PARTKEY], [L_PARTITION_KEY]) \non ps_LINEITEM([L_PARTITION_KEY]);\n```"],"metadata":{}},{"cell_type":"markdown","source":["As DataFrame and Azure SQL Table are both partitioned by `L_PARTITION_KEY`, there isn't much left to do and the connector will take care of everything for us. `tableLock` must be set to `false` to avoid table lock that will prevent parallel partitioned load. Thanks to partitions, acquired locks will not interfere with each other."],"metadata":{}},{"cell_type":"code","source":["li.write \n  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n  .mode(\"overwrite\")   \n  .option(\"truncate\", \"true\") \n  .option(\"url\", url) \n  .option(\"dbtable\", \"dbo.LINEITEM_LOADTEST\") \n  .option(\"user\", user) \n  .option(\"password\", password) \n  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n  .option(\"tableLock\", \"false\") \n  .option(\"batchsize\", \"100000\") \n  .option(\"schemaCheckEnabled\", \"false\") // needed to avoid clash of NULLable columns vs NON-NULLable colums\n  .save()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["## Load data into a partitioned table with column-store index"],"metadata":{}},{"cell_type":"markdown","source":["Empty the test table if needed, to speed up index deletion\n\n```sql\ntruncate table dbo.[LINEITEM_LOADTEST];\n```\n\nDrop the previously create indexes if needed:\n```sql\ndrop index IXC on dbo.[LINEITEM_LOADTEST];\ndrop index IX1 on dbo.[LINEITEM_LOADTEST];\ndrop index IX2 on dbo.[LINEITEM_LOADTEST];\n```\n\nAnd then create a clustered columnstore index:\n\n```sql\ncreate clustered columnstore index IXCCS on dbo.[LINEITEM_LOADTEST]\non ps_LINEITEM([L_PARTITION_KEY]);\n```"],"metadata":{}},{"cell_type":"markdown","source":["Load data using [columnstore data loading best pratices](https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-data-loading-guidance), by loading 1048576 rows at time, to land directly into a compressed segment. Locking the table must be set to `false` to avoid locking. Data with be loaded in parallel, using as many as Apache Spark workers are available."],"metadata":{}},{"cell_type":"code","source":["li.write \n  .format(\"com.microsoft.sqlserver.jdbc.spark\") \n  .mode(\"overwrite\")   \n  .option(\"truncate\", \"true\") \n  .option(\"url\", url) \n  .option(\"dbtable\", \"dbo.LINEITEM_LOADTEST\") \n  .option(\"user\", user) \n  .option(\"password\", password) \n  .option(\"reliabilityLevel\", \"BEST_EFFORT\") \n  .option(\"tableLock\", \"false\") \n  .option(\"batchsize\", \"1048576\") \n  .option(\"schemaCheckEnabled\", \"false\")\n  .save()"],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"02-load-into-partitioned-table","notebookId":1536696850337469},"nbformat":4,"nbformat_minor":0}