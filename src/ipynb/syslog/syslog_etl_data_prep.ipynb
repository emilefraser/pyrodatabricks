{"cells":[{"cell_type":"markdown","source":["### Data Prep\nThis notebook will cover how to access existing data in S3 in a particular bucket."],"metadata":{}},{"cell_type":"code","source":["%run \"/Users/mwc@databricks.com/_helper.py\""],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["ACCESS_KEY = \"[REPLACE_WITH_ACCESS_KEY]\"\nSECRET_KEY = \"[REPLACE_WITH_SECRET_KEY]\"\nENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\nAWS_BUCKET_NAME = get_bucket_name()\nMOUNT_NAME = \"mwc\"\n\n# Mount S3 bucket \ntry:\n  dbutils.fs.ls(\"/mnt/%s\" % MOUNT_NAME)\nexcept:\n  print \"Mount not found. Attempting to mount...\"\n  dbutils.fs.mount(\"s3n://%s:%s@%s/\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["display(dbutils.fs.ls(\"/mnt/mwc\"))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%fs head dbfs:/mnt/mwc/accesslog/databricks.com-access.log\t"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["* Create an external table against the access log data where we define a regular expression format as part of the serializer/deserializer (SerDe) definition.  \n* Instead of writing ETL logic to do this, our table definition handles this.\n* Original Format: %s %s %s [%s] \\\"%s %s HTTP/1.1\\\" %s %s\n* Example Web Log Row \n * 10.0.0.213 - 2185662 [14/Aug/2015:00:05:15 -0800] \"GET /Hurricane+Ridge/rss.xml HTTP/1.1\" 200 288"],"metadata":{}},{"cell_type":"code","source":["%sql\nDROP TABLE IF EXISTS accesslog;\nCREATE EXTERNAL TABLE accesslog (\n  ipaddress STRING,\n  clientidentd STRING,\n  userid STRING,\n  datetime STRING,\n  method STRING,\n  endpoint STRING,\n  protocol STRING,\n  responseCode INT,\n  contentSize BIGINT,\n  referrer STRING,\n  agent STRING,\n  duration STRING,\n  ip1 STRING,\n  ip2 STRING,\n  ip3 STRING,\n  ip4 STRING\n)\nROW FORMAT\n  SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\nWITH SERDEPROPERTIES (\n  \"input.regex\" = '^(\\\\S+) (\\\\S+) (\\\\S+) \\\\[([\\\\w:/]+\\\\s[+\\\\-]\\\\d{4})\\\\]  \\\\\"(\\\\S+) (\\\\S+) (\\\\S+)\\\\\" (\\\\d{3}) (\\\\d+) \\\\\"(.*)\\\\\" \\\\\"(.*)\\\\\" (\\\\S+) \\\\\"(\\\\S+), (\\\\S+), (\\\\S+), (\\\\S+)\\\\\"'\n)\nLOCATION \n  \"/mnt/mwc/accesslog/\""],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql select ipaddress, datetime, method, endpoint, protocol, responsecode, agent from accesslog limit 10;"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Obtain ISO-3166-1 Three Letter Country Codes from IP address\n* Extract out the distinct set of IP addresses from the Apache Access logs\n* Make a REST web service call to freegeoip.net to get the two-letter country codes based on the IP address\n * This creates the **mappedIP2** DataFrame where the schema is encoded.\n* Create a DataFrame to extract out a mapping between 2-letter code, 3-letter code, and country name\n * This creates the **countryCodesDF** DataFrame where the schema is inferred\n* Join these two data frames together and select out only the four columns needed to create the **mappedIP3** DataFrame"],"metadata":{}},{"cell_type":"code","source":["%sql \nDROP TABLE IF EXISTS distinct_ips;\ncreate table distinct_ips as select distinct ip1 from accesslog where ip1 is not null; \nselect count(*) from distinct_ips; "],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["import urllib2 \nimport json\n\napi_key = get_ip_loc_api_key()\n\ndef getCCA2(ip):\n  url = 'http://api.db-ip.com/addrinfo?addr=' + ip + '&api_key=%s' % api_key\n  str = json.loads(urllib2.urlopen(url).read())\n  return str['country'].encode('utf-8')\n\nsqlContext.udf.register(\"mapCCA2\", getCCA2)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["%sql \nDROP TABLE IF EXISTS mapIps;\nCREATE TABLE mapIps AS SELECT ip1 AS ip, mapCCA2(ip1) AS cca2 FROM distinct_ips;"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["%sql SELECT * FROM mapIps LIMIT 40"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["from pyspark.sql import SQLContext, Row\nfrom pyspark.sql.types import *\n\n\nfields = sc.textFile(\"/mnt/mwc/countrycodes/\").map(lambda l: l.split(\",\"))\ncountrycodes = fields.map(lambda x: Row(cn=x[0], cca2=x[1], cca3=x[2]))\nsqlContext.createDataFrame(countrycodes).registerTempTable(\"countryCodes\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["%sql \nSELECT * FROM countryCodes LIMIT 20"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["%sql \nSELECT ip, `mapIps`.cca2 as cca2, `countryCodes`.cca3 as cca3, cn FROM mapIps LEFT OUTER JOIN countryCodes where mapIps.cca2 = countryCodes.cca2"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## Identity the Browser and OS information \n* Extract out the distinct set of user agents from the Apache Access logs\n* Use the Python Package [user-agents](https://pypi.python.org/pypi/user-agents) to extract out Browser and OS information from the User Agent strring\n* For more information on installing pypi packages in Databricks, refer to [Databricks Guide > Product Overview > Libraries](https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#02%20Product%20Overview/07%20Libraries.html)"],"metadata":{}},{"cell_type":"code","source":["from user_agents import parse\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import udf\n\n# Convert None to Empty String\ndef xstr(s): \n  if s is None: \n    return '' \n  return str(s)\n\n# Create UDFs to extract out Browser Family and OS Family information\ndef browserFamily(ua_string) : return xstr(parse(xstr(ua_string)).browser.family)\ndef osFamily(ua_string) : return xstr(parse(xstr(ua_string)).os.family)\n\nsqlContext.udf.register(\"browserFamily\", browserFamily)\nsqlContext.udf.register(\"osFamily\", osFamily)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["%sql\nDROP TABLE IF EXISTS userAgentTable;\nDROP TABLE IF EXISTS userAgentInfo; \nCREATE TABLE userAgentTable AS SELECT DISTINCT agent FROM accesslog; \nCREATE TABLE userAgentInfo AS SELECT agent, osFamily(agent) as OSFamily, browserFamily(agent) as browserFamily FROM userAgentTable; "],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["%sql \nSELECT browserFamily, count(1) FROM UserAgentInfo group by browserFamily"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["## UserID, Date, and Joins\nTo make finish basic preparation of these web logs, we will do the following: \n* Convert the Apache web logs date information\n* Create a userid based on the IP address and User Agent (these logs do not have a UserID)\n * We are generating the UserID (a way to uniquify web site visitors) by combining these two columns\n* Join back to the Browser and OS information as well as Country (based on IP address) information\n* Also include call to udfWeblog2Time function to convert the Apache web log date into a Spark SQL / Hive friendly format (for session calculations below)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import DateType\nfrom pyspark.sql.functions import udf\nimport time\n\n# weblog2Time function\n#   Input: 04/Nov/2015:08:15:00 +0000\n#   Output: 2015-11-04 08:15:00\ndef weblog2Time(weblog_timestr):\n  weblog_time = time.strptime(weblog_timestr, \"%d/%b/%Y:%H:%M:%S +0000\")\n  weblog_t = time.mktime(weblog_time)\n  return time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime(weblog_t))\n\n# Register the UDF\nsqlContext.udf.register(\"weblog2Time\", weblog2Time)\n"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["From here I'll use the SQL notebook to continue the SQL analysis, but refer back to this notebook for any user defined functions."],"metadata":{}}],"metadata":{"name":"Syslog ETL - Data Prep","notebookId":237711},"nbformat":4,"nbformat_minor":0}