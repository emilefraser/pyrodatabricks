{"cells":[{"cell_type":"markdown","source":["# 03a - Parallel Switch-In Load Into Partitioned Table\n\nIf you have to load data into a table that is also actively used by users, you cannot just run a bulk copy operation on such table. If you plan to use `tableLock` option, users will not be able to access data for the whole duration of the bulk load; even if you don't plan to use `tableLock` option, a bulk load operation will still impact and interfere with conccurrent operations running on the table partition.\n\nTo get more details on partitioning, take a look at the `02-load-into-partitioned-table` notebook.\n\nThe solution to be able to bulk load data and at the same time have the table usable by applications and users is simple: load another table instead, and then \"switch-in\" that table into the target one. More details on this pattern can be found in [this post](https://www.cathrinewilhelmsen.net/2015/04/19/table-partitioning-in-sql-server-partition-switching/) written by the Data Platform MVP Cathrine Wilhelmsen. \n\nBeside improving concurrency during bulk load operation, you also have another benefit that can be very useful. When not using the switch-in ability just discussed, it's usually better to load the table with indexes already created, as for very big tables, creating an index can completely drain all the resources avaiable to your Azure SQL database. By using this tecnique you are actually using a \"divide-et-impera\" approach, so that you can load data into a staging table with no indexes, where you'll have the best load performance possible, and then create the needed index later, with much lower impact on resources. The lower resource impact is due to the fact that you are only load data that will go into a single partition, not the whole table, and thus should be smaller and much more manageable. By repeating this process for all partitions you need to load, you can load data without impacting to much on Azure SQL resources and thus query performances.\n\nDue to the fact that Apache Spark RDD partitions and Azure SQL partitions are in a 1:N relationship, is not possibile for the Azure SQL Connector to easily determine which staging table should be used and how to do the switch-in. Luckily we can do this operation manually, using a [well documented technique](https://docs.databricks.com/notebooks/notebook-workflows.html), helping Apache Spark to maximize parallelism to load Azure SQL partitions.\n\nThe sample is using the new sql-spark-connector (https://github.com/microsoft/sql-spark-connector). Make sure you have installed it before running this notebook.\n\n## Notes on terminology\n\nThe term \"row-store\" is used to identify and index that is not using the [column-store layout](https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview) to store its data.\n\n## Sample\n\nThis notebook is used to parallelize the work done by another notebook (`03b-parallel-switch-in-load-into-partitioned-table-single.ipynb`), that is actually the one loading the data into a staging table via bulk copy and than doing the switch-in operation.  \n\n## Supported Azure Databricks Versions\n\nDatabricks supported versions: Spark 2.4.5 and Scala 2.11"],"metadata":{}},{"cell_type":"markdown","source":["# Create Target Table\nCreate table and its indexes"],"metadata":{}},{"cell_type":"markdown","source":["Make sure you create on your Azure SQL the following `LINEITEM` table, partitioned by `L_PARTITION_KEY`:\n\n```sql\ncreate partition function pf_LINEITEM(int)\nas range left for values \n(\n\t199201,199202,199203,199204,199205,199206,199207,199208,199209,199210,199211,199212,\n\t199301,199302,199303,199304,199305,199306,199307,199308,199309,199310,199311,199312,\n\t199401,199402,199403,199404,199405,199406,199407,199408,199409,199410,199411,199412,\n\t199501,199502,199503,199504,199505,199506,199507,199508,199509,199510,199511,199512,\n\t199601,199602,199603,199604,199605,199606,199607,199608,199609,199610,199611,199612,\n\t199701,199702,199703,199704,199705,199706,199707,199708,199709,199710,199711,199712,\n\t199801,199802,199803,199804,199805,199806,199807,199808,199809,199810\n);\n\ncreate partition scheme ps_LINEITEM\nas partition pf_LINEITEM\nall to ([Primary])\n;\n\ncreate table [dbo].[LINEITEM_LOADTEST]\n(\n\t[L_ORDERKEY] [int] not null,\n\t[L_PARTKEY] [int] not null,\n\t[L_SUPPKEY] [int] not null,\n\t[L_LINENUMBER] [int] not null,\n\t[L_QUANTITY] [decimal](15, 2) not null,\n\t[L_EXTENDEDPRICE] [decimal](15, 2) not null,\n\t[L_DISCOUNT] [decimal](15, 2) not null,\n\t[L_TAX] [decimal](15, 2) not null,\n\t[L_RETURNFLAG] [char](1) not null,\n\t[L_LINESTATUS] [char](1) not null,\n\t[L_SHIPDATE] [date] not null,\n\t[L_COMMITDATE] [date] not null,\n\t[L_RECEIPTDATE] [date] not null,\n\t[L_SHIPINSTRUCT] [char](25) not null,\n\t[L_SHIPMODE] [char](10) not null,\n\t[L_COMMENT] [varchar](44) not null,\n\t[L_PARTITION_KEY] [int] not null\n) on ps_LINEITEM([L_PARTITION_KEY])\n;\n\ncreate clustered index IXC on dbo.[LINEITEM_LOADTEST] ([L_COMMITDATE]) \non ps_LINEITEM([L_PARTITION_KEY]);\n\ncreate unique nonclustered index IX1 on dbo.[LINEITEM_LOADTEST] ([L_ORDERKEY], [L_LINENUMBER], [L_PARTITION_KEY]) \non ps_LINEITEM([L_PARTITION_KEY]);\n\ncreate nonclustered index IX2 on dbo.[LINEITEM_LOADTEST] ([L_PARTKEY], [L_PARTITION_KEY]) \non ps_LINEITEM([L_PARTITION_KEY]);\n```"],"metadata":{}},{"cell_type":"markdown","source":["## Create support function\nTo be able to execute a switch-in load, parallel load must be managed manually, as T-SQL code must be execute before and after each Azure SQL partition (not Dataframe partition! Remember that a Dataframe partition can target multiple Azure SQL partitions) has been loaded bia bulk load operation. By using the [tecnique explained in the official Databricks documentation](https://docs.databricks.com/notebooks/notebook-workflows.html#api) it is possibile to execute a notebook in parallel, by implementing the following function."],"metadata":{}},{"cell_type":"code","source":["import scala.concurrent.{Future, Await}\nimport scala.concurrent.duration._\nimport scala.util.control.NonFatal\n\ncase class NotebookData(path: String, timeout: Int, parameters: Map[String, String] = Map.empty[String, String])\n\ndef parallelNotebooks(notebooks: Seq[NotebookData]): Future[Seq[String]] = {\n  import scala.concurrent.{Future, blocking, Await}\n  import java.util.concurrent.Executors\n  import scala.concurrent.ExecutionContext\n  import com.databricks.WorkflowException\n\n  val numNotebooksInParallel = 4 \n  // If you create too many notebooks in parallel the driver may crash when you submit all of the jobs at once. \n  // This code limits the number of parallel notebooks.\n  implicit val ec = ExecutionContext.fromExecutor(Executors.newFixedThreadPool(numNotebooksInParallel))\n  val ctx = dbutils.notebook.getContext()\n  \n  Future.sequence(\n    notebooks.map { notebook => \n      Future {\n        dbutils.notebook.setContext(ctx)\n        if (notebook.parameters.nonEmpty)\n          dbutils.notebook.run(notebook.path, notebook.timeout, notebook.parameters)\n        else\n          dbutils.notebook.run(notebook.path, notebook.timeout)\n      }\n      .recover {\n        case NonFatal(e) => s\"ERROR: ${e.getMessage}\"\n      }\n    }\n  )\n}"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import scala.concurrent.{Future, Await}\nimport scala.concurrent.duration._\nimport scala.util.control.NonFatal\ndefined class NotebookData\nparallelNotebooks: (notebooks: Seq[NotebookData])scala.concurrent.Future[Seq[String]]\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## Run Parallel Load"],"metadata":{}},{"cell_type":"markdown","source":["Create a Sequence with Azure SQL partitions to be loaded is stored"],"metadata":{}},{"cell_type":"code","source":["import spark.implicits._\nimport org.apache.spark.sql._\n\ncase class partitionToProcess(partitionKey:Int)\n\nval ptp = Seq(\n    partitionToProcess(199702),\n    partitionToProcess(199703),\n    partitionToProcess(199704),\n    partitionToProcess(199706),\n    partitionToProcess(199707),\n    partitionToProcess(199708)\n)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import spark.implicits._\nimport org.apache.spark.sql._\ndefined class partitionToProcess\nptp: Seq[partitionToProcess] = List(partitionToProcess(199702), partitionToProcess(199703), partitionToProcess(199704), partitionToProcess(199706), partitionToProcess(199707), partitionToProcess(199708))\n</div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["Execute in parallel several instances of the notebook that load a specific partition, using a different partition key for each instance"],"metadata":{}},{"cell_type":"code","source":["import scala.concurrent.Await\nimport scala.concurrent.duration._\nimport scala.language.postfixOps\n\nval timeOut = 600 // seconds\nval ipynb = \"./03b-parallel-switch-in-load-into-partitioned-table-single\"\n\nval notebooks = ptp.map(p => NotebookData(ipynb, timeOut, Map(\"partitionKey\" -> p.partitionKey.toString)))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import scala.concurrent.Await\nimport scala.concurrent.duration._\nimport scala.language.postfixOps\ntimeOut: Int = 600\nipynb: String = ./03b-parallel-switch-in-load-into-partitioned-table-single\nnotebooks: Seq[NotebookData] = List(NotebookData(./03b-parallel-switch-in-load-into-partitioned-table-single,600,Map(partitionKey -&gt; 199702)), NotebookData(./03b-parallel-switch-in-load-into-partitioned-table-single,600,Map(partitionKey -&gt; 199703)), NotebookData(./03b-parallel-switch-in-load-into-partitioned-table-single,600,Map(partitionKey -&gt; 199704)), NotebookData(./03b-parallel-switch-in-load-into-partitioned-table-single,600,Map(partitionKey -&gt; 199706)), NotebookData(./03b-parallel-switch-in-load-into-partitioned-table-single,600,Map(partitionKey -&gt; 199707)), NotebookData(./03b-parallel-switch-in-load-into-partitioned-table-single,600,Map(partitionKey -&gt; 199708)))\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["val res = parallelNotebooks(notebooks)\n\nAwait.result(res, (timeOut * ptp.size seconds)) // this is a blocking call.\n\nres.value"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res: scala.concurrent.Future[Seq[String]] = Future(Success(List(199702, 199703, 199704, 199706, 199707, 199708)))\nres3: Option[scala.util.Try[Seq[String]]] = Some(Success(List(199702, 199703, 199704, 199706, 199707, 199708)))\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":12}],"metadata":{"name":"03a-parallel-switch-in-load-into-partitioned-table-many","notebookId":964636935775876},"nbformat":4,"nbformat_minor":0}